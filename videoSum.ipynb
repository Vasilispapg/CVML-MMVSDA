{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 19:52:19.345393: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from sklearn.cluster import KMeans\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from math import floor,sqrt,ceil,log2\n",
    "from moviepy.editor import VideoFileClip\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import h5py\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import objectDetection as od\n",
    "from tabulate import tabulate\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Feature Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Video Processing and Frame Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, frame_rate=1):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    count = 0\n",
    "    success = True\n",
    "    frames = []\n",
    "    \n",
    "    while success:\n",
    "        success, image = video.read()\n",
    "        if count % frame_rate == 0 and success:\n",
    "            frames.append(image)\n",
    "        count += 1\n",
    "\n",
    "    video.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Audio Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_audio_from_video(video_path, output_audio_path):\n",
    "    video = VideoFileClip(video_path)\n",
    "    audio = video.audio\n",
    "    audio.write_audiofile(output_audio_path)\n",
    "    video.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c.Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features_for_each_frame(audio_path, frame_rate,num_frames):\n",
    "    y, sr = librosa.load(audio_path)\n",
    "\n",
    "    # Calculate the number of audio samples per video frame\n",
    "    samples_per_frame = sr / frame_rate\n",
    "\n",
    "    # Initialize an array to store MFCCs for each frame\n",
    "    mfccs_per_frame = []\n",
    "\n",
    "    # Iterate over each frame and extract corresponding MFCCs\n",
    "    for frame in range(int(len(y) / samples_per_frame)):\n",
    "        start_sample = int(frame * samples_per_frame)\n",
    "        end_sample = int((frame + 1) * samples_per_frame)\n",
    "\n",
    "        # Ensure the end sample does not exceed the audio length\n",
    "        end_sample = min(end_sample, len(y))\n",
    "\n",
    "        # Extract MFCCs for the current frame's audio segment\n",
    "        mfccs_current_frame = librosa.feature.mfcc(y=y[start_sample:end_sample], sr=sr, n_mfcc=13)\n",
    "        mfccs_processed = np.mean(mfccs_current_frame.T, axis=0)\n",
    "        mfccs_per_frame.append(mfccs_processed)\n",
    "    print(np.array(mfccs_per_frame).shape)\n",
    "\n",
    "    if(len(mfccs_per_frame)>num_frames):\n",
    "        return mfccs_per_frame[:num_frames]\n",
    "    return mfccs_per_frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Feature Extraction (example with visual features using a CNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Sequential\n",
    "import cv2\n",
    "import h5py\n",
    "\n",
    "# model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "# Load the weights from the downloaded file\n",
    "base_model = VGG16(weights=None, include_top=False)\n",
    "weights_path = 'vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5' # Replace with the actual path\n",
    "base_model.load_weights(weights_path)\n",
    "\n",
    "# Create a new Sequential model and add the VGG16 base model\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "\n",
    "def extract_visual_features(frames):\n",
    "    features = []\n",
    "    for frame in frames:\n",
    "        if frame is not None:\n",
    "            img = cv2.resize(frame, (224, 224))  # Resize frame to 224x224\n",
    "            img = img_to_array(img)              # Convert to array\n",
    "            img = np.expand_dims(img, axis=0)    # Add batch dimension\n",
    "            img = preprocess_input(img)          # Preprocess for VGG16\n",
    "            \n",
    "            feature = model.predict(img,use_multiprocessing=True,workers=4)\n",
    "            features.append(feature.flatten())\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect Audio/Annotation/Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotation To List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation2List(annotation_features):\n",
    "    # Make the string '1,1,1,3,2,2,4,4,1' to float list\n",
    "    annotation_float_array=[]\n",
    "    for annotation in annotation_features:\n",
    "        \n",
    "        if isinstance(annotation,str):\n",
    "            annotation = annotation.split(',')\n",
    "        if isinstance(annotation,list):\n",
    "            for anno in annotation:\n",
    "                annotation_float_array.append(float(anno))\n",
    "        else:\n",
    "            annotation_float_array.append(float(annotation))\n",
    "    return annotation_float_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title Tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfTitle(info_file,video_path):\n",
    "    info_df = pd.read_csv(info_file, sep='\\t')\n",
    "    info_video=info_df[info_df['video_id']==video_path.split('/')[-1].split('.')[0]]\n",
    "    title=info_video['title'].values[0]\n",
    "    # Preprocess titles and extract features (TF-IDF or word embeddings)\n",
    "    # For TF-IDF:\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    title_features = tfidf_vectorizer.fit_transform([title]).toarray()\n",
    "    return title_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractData(video_path, anno_file, info_file,flag_to_extract):\n",
    "    \n",
    "    return_data=[]\n",
    "    # Extract frames from the video\n",
    "    if(flag_to_extract[0]):\n",
    "        frames = extract_frames(video_path)\n",
    "        return_data.append(['frames',frames])\n",
    "    else:\n",
    "        return_data.append(None)\n",
    "\n",
    "    # # Extract visual features\n",
    "    if(flag_to_extract[1]):\n",
    "        visual_features = extract_visual_features(frames) \n",
    "        return_data.append(['visual',visual_features])\n",
    "    else:\n",
    "        return_data.append(None)\n",
    "\n",
    "    # Extract audio\n",
    "    if(flag_to_extract[2]):\n",
    "        audio_output_path = 'datasets/extractedAudio/extracted_audio.wav'\n",
    "        extract_audio_from_video(video_path, audio_output_path) \n",
    "\n",
    "        # Extract audio features\n",
    "        audio_features = extract_audio_features_for_each_frame(audio_output_path,30,len(frames))\n",
    "        return_data.append(['audio',audio_features])\n",
    "    else:\n",
    "        return_data.append(None)\n",
    "\n",
    "    # Load titles from info file\n",
    "    if(flag_to_extract[3]):\n",
    "        title_features = tfTitle(info_file,video_path)\n",
    "        print(title_features.shape)\n",
    "        print(title_features)\n",
    "        return_data.append(['title',title_features])\n",
    "    else:\n",
    "        return_data.append(None)\n",
    "\n",
    "    return return_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans and feature connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(objects_in_frame, unique_objects):\n",
    "    # Handle None or empty values as 'no_object'\n",
    "    object_array = ['no_object' if x is None or len(x) == 0 else x for x in objects_in_frame]\n",
    "\n",
    "    # Reshape the array for OneHotEncoder\n",
    "    object_array = np.array(object_array).reshape(-1, 1)\n",
    "\n",
    "    # Initialize OneHotEncoder\n",
    "    encoder = OneHotEncoder(sparse=False,dtype=float, categories=[unique_objects])\n",
    "\n",
    "    # Fit and transform the data\n",
    "    one_hot_encoded = encoder.fit_transform(object_array)\n",
    "    return one_hot_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detect objects and return padded encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectObjects(frames, yolo_model, classes, objects=None,video=None):\n",
    "    if objects is None:\n",
    "        print('Detecting objects in frames...')\n",
    "        yolo_model, classes = loadYOLOv5()\n",
    "        \n",
    "        objects = od.detect_objects_in_all_frames(frames, yolo_model, classes)\n",
    "        saveData('objects',objects,video)\n",
    "\n",
    "    # Here, taking the first detected object\n",
    "    objects = [frame_objects[0] if frame_objects else 'None' for frame_objects in objects]\n",
    "\n",
    "    # Generate unique objects and include 'no_object'\n",
    "    unique_objects = sorted(set(objects + ['no_object']))\n",
    "\n",
    "    # One-hot encoding of objects\n",
    "    encoded_objects = one_hot_encode(objects, unique_objects)\n",
    "\n",
    "    # Padding not required as one-hot encoding ensures consistent vector length\n",
    "    \n",
    "    return encoded_objects, objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### plot Data $Remove it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(combined_features_normalized, kmeans):\n",
    "\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    plt.scatter(combined_features_normalized[:, 0], combined_features_normalized[:, 1], c=kmeans.labels_, cmap='viridis', marker='o', s=5)  # Reduced size\n",
    "\n",
    "    # Add labels to each point\n",
    "    for i, label in enumerate(kmeans.labels_):\n",
    "        plt.text(combined_features_normalized[i, 0], combined_features_normalized[i, 1], str(label), fontsize=8)  # Reduced fontsize\n",
    "\n",
    "    plt.title('KMeans Clustering')\n",
    "    plt.xlabel('PCA Feature 1')\n",
    "    plt.ylabel('PCA Feature 2')\n",
    "    plt.colorbar(label='Cluster Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save Data if ness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def saveData(name,feature,video):\n",
    "    if not os.path.exists(f'video_ext_data/{video}'):\n",
    "        os.makedirs(f'video_ext_data/{video}')\n",
    "    with open(f'video_ext_data/{video}/{name}.pkl', 'wb') as f:\n",
    "        pickle.dump(feature,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Data if ness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def getData(feature,video):\n",
    "    if os.path.exists(f'video_ext_data/{video}/{feature}.pkl'):\n",
    "        with open(f'video_ext_data/{video}/{feature}.pkl', 'rb') as f:\n",
    "            feature = pickle.load(f)\n",
    "        return feature\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiModalKMeans(video_path, anno_file, info_file,n_components=None,num_clusters=None,getDataFlag=False):\n",
    "    \"\"\"\n",
    "    Integrate visual, audio, and annotation features from a video,\n",
    "    and perform clustering on the combined features.\n",
    "\n",
    "    :param video_path: Path to the video file.\n",
    "    :param anno_file: Path to the annotation file.\n",
    "    :param info_file: Path to the info file.\n",
    "    :param num_clusters: Number of clusters to use in KMeans.\n",
    "    :return: Cluster labels for each data point.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract data from video\n",
    "    objects=None\n",
    "    \n",
    "    video=video_path.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    # GetData\n",
    "    objects=getData('objects',video)\n",
    "\n",
    "    \n",
    "    frames=getData('frames',video)\n",
    "    visual_features=getData('visual',video)\n",
    "    audio_features=getData('audio',video)\n",
    "    title_features=getData('title',video)\n",
    "    \n",
    "    flag_to_extract=[True,True,True,True]\n",
    "    \n",
    "    if(frames is not None):\n",
    "        flag_to_extract[0]=False\n",
    "    if(visual_features is not None):\n",
    "        flag_to_extract[1]=False\n",
    "    if(audio_features is not None):\n",
    "        flag_to_extract[2]=False\n",
    "    if(title_features is not None):\n",
    "        flag_to_extract[3]=False\n",
    "    \n",
    "    if not getDataFlag:\n",
    "        # Extract data from video and save it\n",
    "        data=extractData(video_path, anno_file, info_file,flag_to_extract)\n",
    "        # Save extracted Data\n",
    "        for d in data:\n",
    "            if d is not None:\n",
    "                if(d[0]=='objects'):\n",
    "                    objects=d[1]\n",
    "                elif(d[0]=='frames'):\n",
    "                    frames=d[1]\n",
    "                elif(d[0]=='visual'):\n",
    "                    visual_features=d[1]\n",
    "                elif(d[0]=='audio'):\n",
    "                    audio_features=d[1]\n",
    "                elif(d[0]=='title'):\n",
    "                    title_features=d[1]\n",
    "                \n",
    "                saveData(d[0],d[1],video)\n",
    "            \n",
    "    # Combine features with padding\n",
    "    combined_features = []\n",
    "    if(objects is None):\n",
    "        padded_encoded_objects,objects = detectObjects(frames,len(visual_features),len(audio_features),video=video)\n",
    "    else:\n",
    "        padded_encoded_objects,objects = detectObjects(frames,len(visual_features),len(audio_features),objects)\n",
    "    \n",
    "        \n",
    "    for i, frame in enumerate(frames):\n",
    "        # annotation_float_array = annotation2List(annotation_features[i])\n",
    "        combined_feature = np.concatenate([\n",
    "            np.array(visual_features[i], dtype=float),\n",
    "            np.array(audio_features[i], dtype=float),\n",
    "            padded_encoded_objects[i],\n",
    "            np.array(title_features, dtype=float).reshape(-1),\n",
    "        ])\n",
    "        combined_features.append(combined_feature)\n",
    "\n",
    "    # Convert to 2D NumPy array and normalize    \n",
    "    combined_features_normalized = StandardScaler().fit_transform(np.array(combined_features))\n",
    "    \n",
    "    # PCA\n",
    "    if n_components is not None:\n",
    "        pca = PCA(n_components=n_components)\n",
    "        combined_features_reduced = pca.fit_transform(combined_features_normalized)\n",
    "    else:\n",
    "        combined_features_reduced = combined_features_normalized\n",
    "    \n",
    "    # num_clusters= ceil(sqrt(sqrt(len(combined_features_reduced) * 2)))\n",
    "    \n",
    "    \n",
    "    print(\"Number of clusters:\", num_clusters)\n",
    "    \n",
    "    # Perform clustering\n",
    "    print(\"Performing clustering...\")\n",
    "    try:\n",
    "        if num_clusters:\n",
    "            kmeans = KMeans(n_clusters=num_clusters) \n",
    "            kmeans.fit(combined_features_reduced)\n",
    "            plotData(combined_features_reduced,kmeans) #Delete later\n",
    "            \n",
    "            return [frames, kmeans.labels_,objects]\n",
    "    except:\n",
    "        print('Error None number of clusters')\n",
    "    # return the frames and the labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find min cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def getMinClusster(labels):\n",
    "    cluster_counts = Counter(labels)\n",
    "\n",
    "    # Find the cluster with the maximum number of frames\n",
    "    return min(cluster_counts, key=cluster_counts.get),cluster_counts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frames Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_selection(frames, labels):\n",
    "    \"\"\"\n",
    "    Create a summary by selecting frames from the most populous cluster.\n",
    "\n",
    "    :param frames: List of frames.\n",
    "    :param labels: Cluster labels for each frame.\n",
    "    :return: List of frames belonging to the most populous cluster.\n",
    "    \"\"\"\n",
    "    # Count the number of frames in each cluster\n",
    "    max_cluster,cluster_counts = getMinClusster(labels)\n",
    "\n",
    "    # Initialize lists for indices and frames\n",
    "    summary_indices = []  # to measure the importance based on annotation\n",
    "    summary_frames = []\n",
    "\n",
    "    # Iterate and select frames and their indices belonging to the most populous cluster\n",
    "    for index, (frame, label) in enumerate(zip(frames, labels)):\n",
    "        if label == max_cluster:\n",
    "            summary_indices.append(index)\n",
    "            summary_frames.append(frame)\n",
    "\n",
    "    print(f\"Number of frames selected for summary: {len(summary_frames)}, Cluster: {max_cluster}\")\n",
    "    # print all clusters len\n",
    "    print(f\"Cluster counts: {cluster_counts}\")\n",
    "    return summary_frames,summary_indices\n",
    "\n",
    "def frameSelectionForEachCluster(frames):\n",
    "    \"\"\"\n",
    "    Create a summary by selecting frames from the most populous cluster.\n",
    "\n",
    "    :param frames: List of frames.\n",
    "    :param labels: Cluster labels for each frame.\n",
    "    :return: List of frames belonging to the most populous cluster.\n",
    "    \"\"\"\n",
    "    # Initialize lists for indices and frames\n",
    "    summary_indices = []  # to measure the importance based on annotation\n",
    "    summary_frames = []\n",
    "\n",
    "    # Iterate and select frames and their indices belonging to the most populous cluster\n",
    "    for index, (frame) in enumerate(frames):\n",
    "        summary_indices.append(index)\n",
    "        summary_frames.append(frame)\n",
    "\n",
    "    print(f\"Number of frames selected for summary: {len(summary_frames)}\")\n",
    "    # print all clusters len\n",
    "    # print(f\"Cluster counts: {cluster_counts}\")\n",
    "    return summary_frames,summary_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video_from_frames(frames, output_path, frame_rate=30):\n",
    "    if not frames:\n",
    "        print(\"No frames to create a video.\")\n",
    "        return None\n",
    "    # Determine the width and height from the first frame\n",
    "    height, width, layers = frames[0].shape\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, frame_rate, (width, height))\n",
    "\n",
    "    # Write each frame to the video\n",
    "    for frame in frames:\n",
    "        out.write(frame)\n",
    "\n",
    "    # Release the VideoWriter object\n",
    "    out.release()\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load .Mat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_titles(encoded_titles, hdf5_file):\n",
    "    decoded_titles = []\n",
    "    for ref_array in encoded_titles:\n",
    "        # Handle the case where each ref_array might contain multiple references\n",
    "        for ref in ref_array:\n",
    "            # Dereference each HDF5 object reference to get the actual data\n",
    "            title_data = hdf5_file[ref]\n",
    "            # Decode the title\n",
    "            decoded_title = ''.join(chr(char[0]) for char in title_data)\n",
    "            decoded_titles.append(decoded_title)\n",
    "    return decoded_titles\n",
    "\n",
    "\n",
    "def load_mat_file(file_path,videoID):\n",
    "    \"\"\"\n",
    "    Load a .mat file and return its contents.\n",
    "\n",
    "    :param file_path: Path to the .mat file.\n",
    "    :return: Contents of the .mat file.\n",
    "    \"\"\"\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        user_anno_refs=file['tvsum50']['user_anno'][:] # type: ignore\n",
    "        video_refs=file['tvsum50']['video'][:] # type: ignore\n",
    "\n",
    "        decoded_videos = decode_titles(video_refs,file)\n",
    "    \n",
    "        annotations = []        \n",
    "        # Get the index from decoded video list to find the annotation for the video\n",
    "        index = [i for i, x in enumerate(decoded_videos) if x.lower() in videoID.lower()][0]\n",
    "        \n",
    "        # Iterate over each reference\n",
    "        for ref in user_anno_refs: # type: ignore\n",
    "            # Dereference each HDF5 object reference\n",
    "            ref_data = file[ref[0]]\n",
    "\n",
    "            # Convert to NumPy array and add to the annotations list\n",
    "            annotations.append(np.array(ref_data))\n",
    "            \n",
    "        return annotations[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f1score with ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_frame_selection(ground_truth, summary_indices):\n",
    "    \"\"\"\n",
    "    Evaluate the selected frames by comparing them with the ground truth.\n",
    "    \n",
    "    Args:\n",
    "    ground_truth: Ground truth annotations.\n",
    "    summary_indices: Indices of the selected frames.\n",
    "    \n",
    "    Returns:\n",
    "    Average importance score, max importance score, and proportion of frames with high importance score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Evaluate the selected frames\n",
    "    selected_importance_scores = ground_truth[summary_indices]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if selected_importance_scores.size == 0:\n",
    "        average_importance = 0\n",
    "        max_importance = 0\n",
    "        proportion_high_importance = 0\n",
    "    else:\n",
    "        average_importance = np.mean(selected_importance_scores)  # Average importance score\n",
    "        max_importance = np.max(selected_importance_scores)  # Max importance score\n",
    "        # Calculate the proportion of frames with high importance score\n",
    "        proportion_high_importance = np.mean(selected_importance_scores >= np.floor(max_importance))\n",
    "        \n",
    "    return average_importance, max_importance,proportion_high_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluation(ground_truth_path,summary_indices,videoID):\n",
    "    \n",
    "    # Get the ground_truth\n",
    "    ground_truth = np.array(load_mat_file(ground_truth_path, videoID))\n",
    "    \n",
    "    # Find the mean of all annotators\n",
    "    gold_standard = np.mean(ground_truth, axis=0)\n",
    "    \n",
    "    # based on annotators find the avg_importance, max_importance, prop_high_importance\n",
    "    avg_importance, max_importance, prop_high_importance = evaluate_frame_selection(gold_standard, summary_indices)\n",
    "\n",
    "    # Thresholding based on max_importance\n",
    "    threshold=floor(np.mean(avg_importance))\n",
    "    \n",
    "    \n",
    "    # Binary conversion\n",
    "    binary_ground_truth = np.where(gold_standard >= threshold, 1, 0) \n",
    "    \n",
    "    # Selected frames binary conversion\n",
    "    selected_frames_binary = np.zeros_like(binary_ground_truth)\n",
    "    selected_frames_binary[summary_indices] = 1\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = precision_score(binary_ground_truth, selected_frames_binary)\n",
    "    recall = recall_score(binary_ground_truth, selected_frames_binary)  \n",
    "    f1 = f1_score(binary_ground_truth, selected_frames_binary, average='macro')\n",
    "    \n",
    "    # return metrics\n",
    "    return threshold,precision,recall,f1,avg_importance,max_importance,prop_high_importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KnapSack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knapsack_for_video_summary(values, weights, capacity, scale_factor=30):\n",
    "    \"\"\"\n",
    "    Apply the 0/1 Knapsack algorithm to select video segments for summarization.\n",
    "\n",
    "    :param values: List of importance scores for each segment.\n",
    "    :param weights: List of durations for each segment in seconds.\n",
    "    :param capacity: Maximum total duration for the summary in seconds.\n",
    "    :param scale_factor: Factor to scale weights to integers.\n",
    "    :return: Indices of the segments to include in the summary.\n",
    "    \"\"\"\n",
    "    # Scale weights and capacity\n",
    "    weights = [int(w * scale_factor) for w in weights]\n",
    "    capacity = int(capacity * scale_factor)\n",
    "\n",
    "    n = len(values)\n",
    "    K = [[0 for _ in range(capacity + 1)] for _ in range(n + 1)]\n",
    "\n",
    "    # Build table K[][] in a bottom-up manner\n",
    "    for i in range(n + 1):\n",
    "        for w in range(capacity + 1):\n",
    "            if i == 0 or w == 0:\n",
    "                K[i][w] = 0\n",
    "            elif weights[i-1] <= w:\n",
    "                K[i][w] = max(values[i-1] + K[i-1][w-weights[i-1]], K[i-1][w])\n",
    "            else:\n",
    "                K[i][w] = K[i-1][w]\n",
    "\n",
    "    # Find the selected segments\n",
    "    res = K[n][capacity]\n",
    "    w = capacity\n",
    "    selected_indices = []\n",
    "\n",
    "    for i in range(n, 0, -1):\n",
    "        if res <= 0:\n",
    "            break\n",
    "        if res == K[i-1][w]:\n",
    "            continue\n",
    "        else:\n",
    "            selected_indices.append(i-1)\n",
    "            res = res - values[i-1]\n",
    "            w = w - weights[i-1]\n",
    "\n",
    "    selected_indices.reverse()\n",
    "    return selected_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### importance score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_importance_scores(labels, detected_objects):\n",
    "    \"\"\"Here the importance for all the labels together..\n",
    "\n",
    "    \"\"\"\n",
    "    # Count the frequency of each label\n",
    "    label_counts = Counter(labels)\n",
    "\n",
    "    # Assign a basic importance score based on the inverse frequency of cluster labels\n",
    "    base_importance_scores = [1 / label_counts[label] for label in labels]\n",
    "\n",
    "    # Additional importance based on detected objects\n",
    "    object_importance_scores = []\n",
    "    for objects in detected_objects:\n",
    "        if objects:  # if the list is not empty\n",
    "            # Add extra importance for each detected object\n",
    "            object_importance_scores.append(len(objects))\n",
    "        else:\n",
    "            object_importance_scores.append(0)\n",
    "\n",
    "    # Combine base importance scores with object importance scores\n",
    "    max_object_score = max(object_importance_scores) if object_importance_scores else 1\n",
    "    \n",
    "    # Normalize object importance scores for simplicity\n",
    "    normalized_object_scores = [score / max_object_score for score in object_importance_scores]\n",
    "\n",
    "    # Final importance score is a combination of base and object scores\n",
    "    importance_scores = [base + obj for base, obj in zip(base_importance_scores, normalized_object_scores)]\n",
    "    \n",
    "    print(\"Average importance score:\", np.mean(importance_scores))\n",
    "    \n",
    "    return importance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_importance_scores_for_cluster(cluster_frames, detected_objects_per_frame):\n",
    "\n",
    "    # Initialize importance scores for the cluster\n",
    "    importance_scores = []\n",
    "\n",
    "    # Calculate object-based importance for each frame in the cluster\n",
    "    for frame_index in cluster_frames:\n",
    "        # Calculate object importance\n",
    "        objects = detected_objects_per_frame[frame_index]\n",
    "        object_importance = len(objects) if objects else 0\n",
    "\n",
    "        # Add object importance to the list\n",
    "        importance_scores.append(object_importance)\n",
    "\n",
    "    # Normalize the importance scores\n",
    "    max_importance = max(importance_scores, default=1)\n",
    "    if max_importance == 0:\n",
    "        max_importance = 1  # To prevent division by zero\n",
    "\n",
    "    normalized_importance_scores = [score / max_importance for score in importance_scores]\n",
    "\n",
    "    return normalized_importance_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map labels with frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_frames_to_labels_with_indices(frames, labels):\n",
    "    label_frame_dict = {}\n",
    "    for label, (frame_index, frame) in zip(labels, enumerate(frames)):\n",
    "        if label not in label_frame_dict:\n",
    "            label_frame_dict[label] = []\n",
    "        label_frame_dict[label].append((frame_index, frame))\n",
    "    return label_frame_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Delete pkl Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deletePKLfiles(video):\n",
    "    dirpkl='video_ext_data/'+video+'/'\n",
    "    if(os.path.exists(dirpkl+'frames.pkl')):\n",
    "        os.remove(dirpkl+\"frames.pkl\")\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to create summary video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_path='datasets/ydata-tvsum50-v1_1/data/ydata-tvsum50-anno.tsv'\n",
    "info_path='datasets/ydata-tvsum50-v1_1/data/ydata-tvsum50-info.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path='datasets/ydata-tvsum50-v1_1/video/'\n",
    "summary_video_path='datasets/summary_videos/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_path='datasets/ydata-tvsum50-v1_1/ground_truth/ydata-tvsum50.mat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get the list of the videos in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_list = [video for video in os.listdir(video_path) if video.endswith('.mp4')]  # List comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Yolo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def loadYOLOv5():\n",
    "    # Load the model\n",
    "    yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True,)\n",
    "\n",
    "    with open(\"yolo/coco.names\", \"r\") as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "    return yolo_model,classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_rate = 30  # or whatever your frame rate is\n",
    "frame_duration = 1 / frame_rate  # Duration of each frame in seconds\n",
    "capacity = 15  # 15 seconds summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for videoSummarizion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def videoSumm(annotation_path=None, info_path=None, video_path=None, summary_video_path=None,video_list=None):\n",
    "    for video in video_list: \n",
    "        if(os.path.exists(f'{summary_video_path}{video}')):\n",
    "            continue\n",
    "        max_model=None\n",
    "        getDataFlag=False\n",
    "        bet_num_clusters=None\n",
    "        num_clusters=[3,5,7,9,11,13,15]\n",
    "        getDataFlag=False\n",
    "        \n",
    "        for num in num_clusters:\n",
    "            frames,labels,objects = MultiModalKMeans(video_path+video, annotation_path, info_path,n_components=2,num_clusters=num,getDataFlag=getDataFlag)\n",
    "            \n",
    "            # For the next iteration we don't need to extract the data again\n",
    "            getDataFlag=True\n",
    "            \n",
    "            \"\"\" USE THIS IN CASE OF CREATING VIDEO FOR EACH CLUSTER\"\"\"\n",
    "            cluster_dict = map_frames_to_labels_with_indices(frames, labels)\n",
    "            \n",
    "            # keep track of scores\n",
    "            scores = []\n",
    "            model_videos=[]\n",
    "            for index, cluster in cluster_dict.items():\n",
    "                print(\"Calculate Cluster:\", index)\n",
    "                # Extract just the frame indices for importance score calculation\n",
    "                cluster_frame_indices = [frame_index for frame_index, _ in cluster]\n",
    "\n",
    "                # Calculate importance scores for this cluster\n",
    "                importance = calculate_importance_scores_for_cluster(cluster_frame_indices, objects)\n",
    "\n",
    "                # Create a list of durations, one for each frame in the cluster\n",
    "                durations = [frame_duration] * len(importance)\n",
    "\n",
    "                # Apply the knapsack algorithm\n",
    "                summary_indices = knapsack_for_video_summary(importance, durations, capacity)\n",
    "\n",
    "                # Select Representative Frames (extracting frame part from each tuple)\n",
    "                summary_frames = [cluster[i][1] for i in summary_indices]  # [1] extracts the frame from (frame_index, frame)\n",
    "                \n",
    "                # Extract original indices for evaluation\n",
    "                # i want to extract the original indices from the cluster cause the summary_indices are based on the all frames\n",
    "                # so i need to extract the original indices from each cluster\n",
    "                original_indices_for_eval = [cluster[i][0] for i in summary_indices]  # [0] extracts the original frame index\n",
    "\n",
    "                # Evaluate\n",
    "                evaluated_metrics = Evaluation(ground_truth_path, original_indices_for_eval, video.split('.')[0])\n",
    "                \n",
    "                # Append index to metrics\n",
    "                scores.append((str(index),) + evaluated_metrics)\n",
    "                \n",
    "                # Get the model_video based on the index\n",
    "                model_videos.append([str(index),summary_frames,f\"{summary_video_path}{index}-{video}\"])\n",
    "                \n",
    "            # Sort the table of scores based on F1 score\n",
    "            scores.sort(key=lambda x: x[4], reverse=True)\n",
    "            \n",
    "            if((max_model is None )or (max_model[4] < scores[0][4])):\n",
    "                max_model=scores[0]\n",
    "                bet_num_clusters=num\n",
    "                \n",
    "            # print(tabulate(scores, headers=['videoID','Threshold','Precision', 'Recall', 'F1 Score', 'Avg_importance', 'Max_importance', 'Prop_high_importance'], tablefmt='fancy_grid'))\n",
    "\n",
    "        print(\"VIDEO\",video)\n",
    "        print(f\"Best model clusters: {bet_num_clusters}\",max_model)\n",
    "        index, summary_frames, output_path = next((model_video for model_video in model_videos if model_video[0] == max_model[0]), (None, None, None))        \n",
    "        \n",
    "        print(f\"Best model: {index}\",output_path)\n",
    "        \n",
    "        video_name=video.split('.')[0]\n",
    "        \n",
    "        if(not os.path.exists(f'video_ext_data/{video_name}')):\n",
    "            os.makedirs(f'video_ext_data/{video_name}')\n",
    "        with open(f'video_ext_data/{video_name}/model.pkl', 'wb') as f:\n",
    "            pickle.dump(model_videos,f)\n",
    "        with open(f'video_ext_data/{video_name}/max_model.pkl', 'wb') as f:\n",
    "            pickle.dump(max_model,f)\n",
    "        \n",
    "        # fix output_path\n",
    "        output_path=output_path.replace(f'{index}-','')\n",
    "        # Create Summary Video\n",
    "        create_video_from_frames(summary_frames, output_path, 30)\n",
    "        \n",
    "        del model_videos\n",
    "        del scores\n",
    "        del summary_frames\n",
    "        del importance\n",
    "        del cluster_frame_indices\n",
    "        \n",
    "        # Extract data from video the next video\n",
    "        getDataFlag=False\n",
    "        deletePKLfiles(video_name)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for dynamic annotation and video summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoSumm(annotation_path, info_path, video_path, summary_video_path, video_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video          Cluster    Threshold    Precision     Recall    F1 Score    Avg_importance    Max_importance\n",
      "-----------  ---------  -----------  -----------  ---------  ----------  ----------------  ----------------\n",
      "XzYM3PfTM4w          4            3     0.821918  1            0.94801            3.08037              3.25\n",
      "xxdtq8mxegs          1            3     0.622222  0.777778     0.829731           3.17222              3.9\n",
      "37rzWOQsNIw         12            3     1         0.472803     0.815363           3.13274              3.25\n",
      "i3wAGJaaktw          1            3     0.597656  0.425        0.730731           3.10684              3.6\n",
      "JgHubY5Vw3Y          9            3     0.586022  0.404453     0.708441           3.09368              3.8\n",
      "jcoYJXDG9sw          9            3     0.771863  0.308511     0.697048           3.00665              3.05\n",
      "sTEELN-vY30         12            3     1         0.193333     0.647905           3                    3\n",
      "_xMr-HKMfVA          8            3     0.734694  0.15         0.612005           3.40816              3.7\n",
      "eQu1rNs0an0          2            2     0.885362  0.261731     0.601633           2.39206              2.8\n",
      "VuWGsYPqAX8          0            2     0.925414  0.195678     0.581295           2.80539              3.55\n",
      "GsAD1KT1xo8          3            2     0.853659  0.20202      0.522423           2.2939               2.95\n",
      "LRw_obCPUt0          1            2     1         0.161696     0.513068           2.66433              3.55\n",
      "WxtbjNsCQ8A         11            2     1         0.139061     0.508202           2.40133              3.2\n",
      "EYqVtI9YWJA          1            2     0.945355  0.176771     0.503148           2.60237              3.5\n",
      "Yi4Ij2NM7U4          4            2     1         0.112745     0.480024           2.31011              2.75\n",
      "b626MiF1ew4          8            2     0.771134  0.139085     0.468961           2.26918              2.95\n",
      "kLxoNp-UchI          2            2     0.919192  0.164112     0.457134           2.30581              2.7\n",
      "AwmHb44_ouw         13            2     0.975556  0.092753     0.450261           2.53267              3.75\n",
      "E11zDS9XGzg          4            2     0.993377  0.0669643    0.428931           2.35066              2.9\n",
      "JKpqYvAdIsw          1            2     0.611111  0.1375       0.427922           2.09556              3.05\n",
      "NyBmCxDoHJU          5            2     0.827815  0.134457     0.415926           2.23962              3\n",
      "fWutDQy1nnY          8            2     0.996154  0.058877     0.394688           2.63135              3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fol=os.listdir('/Users/macbookpro2017/Desktop/UnsupervisedVideoSummarization/pkl')\n",
    "\n",
    "data=[]\n",
    "for f in fol:\n",
    "    with open('video_ext_data/'+f+'/max_model.pkl','rb') as file:\n",
    "        df=pickle.load(file)\n",
    "        data.append((f,df[0],df[1],df[2],df[3],df[4],df[5],df[6]))\n",
    "\n",
    "# sort\n",
    "data.sort(key=lambda x: x[5], reverse=True)\n",
    "from tabulate import tabulate\n",
    "print(tabulate(data, headers=['video','Cluster','Threshold','Precision', 'Recall', 'F1 Score', 'Avg_importance', 'Max_importance', 'Prop_high_importance']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_anno='datasets/ydata-tvsum50-v1_1/data/ydata-tvsum50-anno.tsv'\n",
    "with open(dir_anno,'r') as file:\n",
    "    annotations=[]\n",
    "    df=pd.read_csv(file,sep='\\t')\n",
    "    for d in df.values:\n",
    "        if(d[0]=='xxdtq8mxegs'):\n",
    "            annotations.append(d[2])\n",
    "            \n",
    "    print(len(annotations[0]))\n",
    "    # print(df[df[0][0]=='0a0XQ4oA1-g'])\n",
    "    # print(df[0][1].values[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DELETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "╒═══════════╤═════════════╤═════════════╤════════════╤════════════╤══════════════════╤══════════════════╤════════════════════════╕\n",
    "│   videoID │   Threshold │   Precision │     Recall │   F1 Score │   Avg_importance │   Max_importance │   Prop_high_importance │\n",
    "╞═══════════╪═════════════╪═════════════╪════════════╪════════════╪══════════════════╪══════════════════╪════════════════════════╡\n",
    "│         2 │           3 │    0.824176 │ 0.714286   │ 0.875598   │          3.06154 │             3.35 │              0.824176  │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         6 │           2 │    1        │ 0.132075   │ 0.48169    │          2.12333 │             2.4  │              1         │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│        10 │           2 │    1        │ 0.0578616  │ 0.411485   │          2.2462  │             2.45 │              1         │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         5 │           1 │    1        │ 0.0651042  │ 0.0611247  │          1.82956 │             2.4  │              0.277778  │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         1 │           1 │    1        │ 0.0651042  │ 0.0611247  │          1.5     │             2.05 │              0.151111  │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         8 │           1 │    1        │ 0.0651042  │ 0.0611247  │          1.92178 │             2.45 │              0.211111  │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         3 │           1 │    1        │ 0.0651042  │ 0.0611247  │          1.66778 │             2.05 │              0.248889  │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         9 │           1 │    1        │ 0.0651042  │ 0.0611247  │          1.461   │             2.15 │              0.0111111 │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         4 │           1 │    1        │ 0.0347222  │ 0.033557   │          1.94958 │             2.55 │              0.441667  │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         7 │           1 │    1        │ 0.0313947  │ 0.0304391  │          1.52258 │             1.65 │              1         │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         0 │           1 │    1        │ 0.00202546 │ 0.00202137 │          1.45    │             1.45 │              1         │\n",
    "╘═══════════╧═════════════╧═════════════╧════════════╧════════════╧══════════════════╧══════════════════╧════════════════════════╛\n",
    "\n",
    "╒═══════════╤═════════════╤═════════════╤═══════════╤════════════╤══════════════════╤══════════════════╤════════════════════════╕\n",
    "│   videoID │   Threshold │   Precision │    Recall │   F1 Score │   Avg_importance │   Max_importance │   Prop_high_importance │\n",
    "╞═══════════╪═════════════╪═════════════╪═══════════╪════════════╪══════════════════╪══════════════════╪════════════════════════╡\n",
    "│         7 │           3 │    0.824176 │ 0.714286  │  0.875598  │          3.06154 │             3.35 │               0.824176 │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         0 │           2 │    0.884444 │ 0.125157  │  0.470639  │          2.324   │             2.95 │               0.884444 │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         4 │           2 │    1        │ 0.117925  │  0.468909  │          2.12133 │             2.4  │               1        │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         3 │           2 │    0.586667 │ 0.0830189 │  0.420579  │          2.232   │             3.05 │               0.133333 │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         9 │           2 │    1        │ 0.0141509 │  0.366062  │          2.14    │             2.25 │               1        │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         6 │           1 │    1        │ 0.0651042 │  0.0611247 │          1.91667 │             2.55 │               0.4      │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         2 │           1 │    1        │ 0.0651042 │  0.0611247 │          1.92889 │             2.45 │               0.211111 │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         1 │           1 │    1        │ 0.0651042 │  0.0611247 │          1.63333 │             2    │               0.133333 │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         5 │           1 │    1        │ 0.0474537 │  0.0453039 │          1.95046 │             2.4  │               0.365854 │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         8 │           1 │    1        │ 0.0231481 │  0.0226244 │          1.48125 │             1.5  │               1        │\n",
    "╘═══════════╧═════════════╧═════════════╧═══════════╧════════════╧══════════════════╧══════════════════╧════════════════════════╛"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results after the video call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "╒═══════════╤═════════════╤═════════════╤════════════╤════════════╤══════════════════╤══════════════════╤════════════════════════╕\n",
    "│   videoID │   Threshold │   Precision │     Recall │  F1 BINARY │   Avg_importance │   Max_importance │   Prop_high_importance │\n",
    "╞═══════════╪═════════════╪═════════════╪════════════╪════════════╪══════════════════╪══════════════════╪════════════════════════╡\n",
    "│         8 │           3 │  0.821229   │ 0.7        │ 0.755784   │          3.06131 │             3.35 │             0.821229   │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         2 │           3 │  0.167939   │ 0.104762   │ 0.129032   │          2.61374 │             3    │             0.167939   │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         4 │           2 │  1          │ 0.0971698  │ 0.177128   │          2.13625 │             2.4  │             1          │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         6 │           2 │  0.437778   │ 0.0619497  │ 0.10854    │          1.92333 │             2.4  │             0.437778   │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         9 │           2 │  1          │ 0.0327044  │ 0.0633374  │          2.6024  │             2.7  │             1          │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         0 │           1 │  1          │ 0.0231481  │ 0.0452489  │          1.48125 │             1.5  │             1          │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         5 │           2 │  0.12       │ 0.0169811  │ 0.0297521  │          1.626   │             2    │             0.12       │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         7 │           1 │  1          │ 0.0157697  │ 0.0310497  │          1.5     │             1.65 │             1          │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│        10 │           2 │  0.192771   │ 0.00503145 │ 0.00980693 │          1.93675 │             2.45 │             0.192771   │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         1 │           2 │  0.00888889 │ 0.00125786 │ 0.00220386 │          1.44733 │             2.15 │             0.00888889 │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         3 │           0 │  0          │ 0          │ 0          │          0       │             0    │             0          │\n",
    "╘═══════════╧═════════════╧═════════════╧════════════╧════════════╧══════════════════╧══════════════════╧════════════════════════╛\n",
    "\n",
    "╒═══════════╤═════════════╤═════════════╤═══════════╤═══════════╤══════════════════╤══════════════════╤════════════════════════╕\n",
    "│   videoID │   Threshold │   Precision │    Recall │ F1 BINARY │   Avg_importance │   Max_importance │   Prop_high_importance │\n",
    "╞═══════════╪═════════════╪═════════════╪═══════════╪═══════════╪══════════════════╪══════════════════╪════════════════════════╡\n",
    "│         8 │           3 │    0.821229 │ 0.7       │ 0.755784  │          3.06131 │             3.35 │             0.821229   │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼───────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         4 │           2 │    1        │ 0.0971698 │ 0.177128  │          2.13625 │             2.4  │             1          │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼───────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         2 │           2 │    0.984733 │ 0.0811321 │ 0.149913  │          2.61374 │             3    │             0.167939   │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼───────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         6 │           1 │    1        │ 0.0651042 │ 0.122249  │          1.92333 │             2.4  │             0.437778   │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼───────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         5 │           1 │    1        │ 0.0651042 │ 0.122249  │          1.626   │             2    │             0.12       │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼───────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         1 │           1 │    1        │ 0.0651042 │ 0.122249  │          1.44733 │             2.15 │             0.00888889 │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼───────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         9 │           2 │    1        │ 0.0327044 │ 0.0633374 │          2.6024  │             2.7  │             1          │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼───────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         0 │           1 │    1        │ 0.0231481 │ 0.0452489 │          1.48125 │             1.5  │             1          │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼───────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         7 │           1 │    1        │ 0.0157697 │ 0.0310497 │          1.5     │             1.65 │             1          │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼───────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│        10 │           1 │    1        │ 0.0120081 │ 0.0237312 │          1.93675 │             2.45 │             0.192771   │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼───────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         3 │           0 │    0        │ 0         │ 0         │          0       │             0    │             0          │\n",
    "╘═══════════╧═════════════╧═════════════╧═══════════╧═══════════╧══════════════════╧══════════════════╧════════════════════════╛\n",
    "\n",
    "╒═══════════╤═════════════╤═════════════╤═══════════╤════════════╤══════════════════╤══════════════════╤════════════════════════╕\n",
    "│   videoID │   Threshold │   Precision │    Recall │   F1 MACRO │   Avg_importance │   Max_importance │   Prop_high_importance │\n",
    "╞═══════════╪═════════════╪═════════════╪═══════════╪════════════╪══════════════════╪══════════════════╪════════════════════════╡\n",
    "│         8 │           3 │    0.821229 │ 0.7       │  0.87061   │          3.06131 │             3.35 │             0.821229   │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         4 │           2 │    1        │ 0.0971698 │  0.449667  │          2.13625 │             2.4  │             1          │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         2 │           2 │    0.984733 │ 0.0811321 │  0.434039  │          2.61374 │             3    │             0.167939   │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         6 │           1 │    1        │ 0.0651042 │  0.0611247 │          1.92333 │             2.4  │             0.437778   │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         5 │           1 │    1        │ 0.0651042 │  0.0611247 │          1.626   │             2    │             0.12       │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         1 │           1 │    1        │ 0.0651042 │  0.0611247 │          1.44733 │             2.15 │             0.00888889 │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         9 │           2 │    1        │ 0.0327044 │  0.385748  │          2.6024  │             2.7  │             1          │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         0 │           1 │    1        │ 0.0231481 │  0.0226244 │          1.48125 │             1.5  │             1          │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         7 │           1 │    1        │ 0.0157697 │  0.0155249 │          1.5     │             1.65 │             1          │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│        10 │           1 │    1        │ 0.0120081 │  0.0118656 │          1.93675 │             2.45 │             0.192771   │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         3 │           0 │    0        │ 0         │  0         │          0       │             0    │             0          │\n",
    "╘═══════════╧═════════════╧═════════════╧═══════════╧════════════╧══════════════════╧══════════════════╧════════════════════════╛\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Me ta etoima frames,labels,objects,visuals,audio\n",
    "dog\n",
    "╒═══════════╤═════════════╤═════════════╤═══════════╤════════════╤══════════════════╤══════════════════╤════════════════════════╕\n",
    "│   videoID │   Threshold │   Precision │    Recall │   F1 Score │   Avg_importance │   Max_importance │   Prop_high_importance │\n",
    "╞═══════════╪═════════════╪═════════════╪═══════════╪════════════╪══════════════════╪══════════════════╪════════════════════════╡\n",
    "│         2 │           2 │    1        │ 0.216411  │  0.507317  │          2.39927 │             2.7  │              1         │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         1 │           2 │    0.792952 │ 0.0811542 │  0.37865   │          2.61388 │             3    │              0.264317  │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         6 │           2 │    1        │ 0.0464382 │  0.351086  │          2.79806 │             2.95 │              1         │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         5 │           2 │    1        │ 0.0302074 │  0.334025  │          2.17164 │             2.7  │              1         │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         0 │           1 │    1        │ 0.0934292 │  0.085446  │          1.57431 │             2.7  │              0.104396  │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         3 │           1 │    1        │ 0.0569815 │  0.0539097 │          1.76059 │             2.5  │              0.0495495 │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         4 │           1 │    1        │ 0.0397844 │  0.0382622 │          1.81742 │             2.05 │              0.387097  │\n",
    "╘═══════════╧═════════════╧═════════════╧═══════════╧════════════╧══════════════════╧══════════════════╧════════════════════════╛\n",
    "\n",
    "cat\n",
    "╒═══════════╤═════════════╤═════════════╤════════════╤════════════╤══════════════════╤══════════════════╤════════════════════════╕\n",
    "│   videoID │   Threshold │   Precision │     Recall │   F1 Score │   Avg_importance │   Max_importance │   Prop_high_importance │\n",
    "╞═══════════╪═════════════╪═════════════╪════════════╪════════════╪══════════════════╪══════════════════╪════════════════════════╡\n",
    "│        10 │           3 │    0.824176 │ 0.714286   │  0.875598  │          3.06154 │             3.35 │             0.824176   │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         1 │           2 │    1        │ 0.142138   │  0.490619  │          2.46726 │             3    │             0.128319   │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         0 │           2 │    1        │ 0.13239    │  0.481971  │          2.12363 │             2.4  │             1          │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         8 │           2 │    0.549889 │ 0.0779874  │  0.414519  │          2.02905 │             3.05 │             0.00221729 │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         4 │           2 │    1        │ 0.0578616  │  0.411485  │          2.2462  │             2.45 │             1          │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         6 │           1 │    1        │ 0.0679977  │  0.0636684 │          1.87564 │             2.25 │             0.12766    │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         7 │           1 │    1        │ 0.0651042  │  0.0611247 │          1.44333 │             1.7  │             1          │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         9 │           1 │    1        │ 0.0597512  │  0.0563823 │          1.87167 │             2.4  │             0.302663   │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         2 │           1 │    1        │ 0.0313947  │  0.0304391 │          1.52258 │             1.65 │             1          │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         3 │           1 │    1        │ 0.0134549  │  0.0132762 │          1.42473 │             1.45 │             1          │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         5 │           1 │    1        │ 0.00491898 │  0.0048949 │          1.55    │             1.55 │             1          │\n",
    "╘═══════════╧═════════════╧═════════════╧════════════╧════════════╧══════════════════╧══════════════════╧════════════════════════╛\n",
    "\n",
    "results= one_hot_encoding or binary encoding == same fscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
