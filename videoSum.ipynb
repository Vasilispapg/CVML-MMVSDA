{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from sklearn.cluster import KMeans\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from math import floor,sqrt\n",
    "from moviepy.editor import VideoFileClip\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import h5py\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotations loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations\n",
    "def load_annotations(anno_file, info_file):\n",
    "    # Read the annotation and info files\n",
    "    annotations = pd.read_csv(anno_file, sep='\\t', header=None)\n",
    "    info = pd.read_csv(info_file, sep='\\t', header=None)\n",
    "    \n",
    "    # Rename columns for better understanding\n",
    "    annotations.columns = ['video_id', 'category', 'importance_score']\n",
    "    info.columns = ['category_code', 'video_id', 'title', 'url', 'length']\n",
    "    \n",
    "    # print( annotations, info)\n",
    "    \n",
    "    return annotations, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Feature Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Video Processing and Frame Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, frame_rate=25):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    count = 0\n",
    "    success = True\n",
    "    frames = []\n",
    "    \n",
    "    while success:\n",
    "        success, image = video.read()\n",
    "        if count % frame_rate == 0 and success:\n",
    "            frames.append(image)\n",
    "        count += 1\n",
    "\n",
    "    video.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Audio Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_audio_from_video(video_path, output_audio_path):\n",
    "    video = VideoFileClip(video_path)\n",
    "    audio = video.audio\n",
    "    audio.write_audiofile(output_audio_path)\n",
    "    video.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c.Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features(audio_path):\n",
    "    \"\"\"\n",
    "    Extract audio features from an audio file, specifically MFCCs.\n",
    "\n",
    "    :param audio_path: Path to the audio file.\n",
    "    :return: Array of MFCCs.\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_path)\n",
    "\n",
    "    # Extract MFCCs from the audio\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)  # You can change n_mfcc based on your needs\n",
    "\n",
    "    # To capture the variation over time, you might compute statistics across MFCCs over time\n",
    "    # Here, we compute the mean of the MFCCs across time\n",
    "    mfccs_processed = np.mean(mfccs.T,axis=0)\n",
    "\n",
    "    return mfccs_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Feature Extraction (example with visual features using a CNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "def extract_visual_features(frames):\n",
    "    features = []\n",
    "    for frame in frames:\n",
    "        if frame is not None:\n",
    "            img = cv2.resize(frame, (224, 224))  # Resize frame to 224x224\n",
    "            img = img_to_array(img)        # Convert to array\n",
    "            img = np.expand_dims(img, axis=0)    # Add batch dimension\n",
    "            img = preprocess_input(img)          # Preprocess for VGG16\n",
    "            \n",
    "            feature = model.predict(img,use_multiprocessing=True,workers=4)\n",
    "            features.append(feature.flatten())\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect Audio/Annotation/Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding Features to fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(visual_features, audio_features, annotation_features):\n",
    "    # inputs are at least 1D arrays\n",
    "    visual_features = [np.array(v, ndmin=1, dtype=float) for v in visual_features]\n",
    "    audio_features = np.array(audio_features, ndmin=1, dtype=float)\n",
    "    annotation_features = [np.array(a, ndmin=1, dtype=float) for a in annotation_features]\n",
    "\n",
    "    # Find the maximum length of the features\n",
    "    max_length = max(\n",
    "        max(v.size for v in visual_features),\n",
    "        audio_features.size,\n",
    "        max(a.size for a in annotation_features)\n",
    "    )\n",
    "\n",
    "    # Pad features to match the maximum length\n",
    "    visual_padded = [np.pad(v, (0, max_length - v.size), 'constant') for v in visual_features]\n",
    "    audio_padded = np.pad(audio_features, (0, max_length - audio_features.size), 'constant')\n",
    "    annotation_padded = [np.pad(a, (0, max_length - a.size), 'constant') for a in annotation_features]\n",
    "\n",
    "    return visual_padded, audio_padded, annotation_padded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotation To List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation2List(annotation_features):\n",
    "    # Make the string '1,1,1,3,2,2,4,4,1' to float list\n",
    "    annotation_float_array=[]\n",
    "    for annotation in annotation_features:\n",
    "        \n",
    "        if isinstance(annotation,str):\n",
    "            annotation = annotation.split(',')\n",
    "        if isinstance(annotation,list):\n",
    "            for anno in annotation:\n",
    "                annotation_float_array.append(float(anno))\n",
    "        else:\n",
    "            annotation_float_array.append(float(annotation))\n",
    "    return annotation_float_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add weights to ignore padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_feature_weighting(combined_features, original_feature_length, original_feature_weight=1, padding_weight=0.1):\n",
    "    \"\"\"\n",
    "    Apply different weights to the original and padded parts of the feature vectors.\n",
    "\n",
    "    :param combined_features: List of combined feature vectors (including padding).\n",
    "    :param original_feature_length: The length of the original (non-padded) part of the feature vectors.\n",
    "    :param original_feature_weight: Weight to be applied to the original features.\n",
    "    :param padding_weight: Weight to be applied to the padded features.\n",
    "    :return: List of weighted feature vectors.\n",
    "    \"\"\"\n",
    "    weighted_combined_features = []\n",
    "\n",
    "    for feature in combined_features:\n",
    "        # Create a weight vector: higher weight for original features, lower for padding\n",
    "        weights = [original_feature_weight] * original_feature_length\n",
    "        weights += [padding_weight] * (len(feature) - original_feature_length)\n",
    "        \n",
    "        # Apply weights to the feature vector\n",
    "        weighted_feature = np.array(feature) * np.array(weights)\n",
    "        weighted_combined_features.append(weighted_feature)\n",
    "\n",
    "    return weighted_combined_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractData(video_path, anno_file, info_file):\n",
    "    # Extract frames from the video\n",
    "    frames = extract_frames(video_path) \n",
    "\n",
    "    # # Extract visual features\n",
    "    visual_features = extract_visual_features(frames) \n",
    "\n",
    "    # # Extract audio\n",
    "    # audio_output_path = 'datasets/extractedAudio/extracted_audio.wav'\n",
    "    # extract_audio_from_video(video_path, audio_output_path) \n",
    "\n",
    "    # # Extract audio features\n",
    "    # audio_features = extract_audio_features(audio_output_path) \n",
    "\n",
    "    # # Load annotations\n",
    "    annotations, info = load_annotations(anno_file, info_file)\n",
    "    \n",
    "    # return visual_features, frames\n",
    "    return visual_features,frames\n",
    "    return annotations,audio_features,visual_features,frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compact_non_zeros(array):\n",
    "    \"\"\"\n",
    "    Move all non-zero elements of an array to the front, keeping their order.\n",
    "\n",
    "    :param array: Input array.\n",
    "    :return: Compacted array with non-zeros at the front.\n",
    "    \"\"\"\n",
    "    non_zeros = array[array != 0]\n",
    "    if len(non_zeros) == 0:  # If the array is all zeros\n",
    "        print(\"All zeros\")\n",
    "        return array\n",
    "    return np.pad(non_zeros, (0, len(array) - len(non_zeros)), 'constant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kmeans and feature connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_and_cluster_features(video_path, anno_file, info_file, num_clusters=None):\n",
    "    \"\"\"\n",
    "    Integrate visual, audio, and annotation features from a video,\n",
    "    and perform clustering on the combined features.\n",
    "\n",
    "    :param video_path: Path to the video file.\n",
    "    :param anno_file: Path to the annotation file.\n",
    "    :param info_file: Path to the info file.\n",
    "    :param num_clusters: Number of clusters to use in KMeans.\n",
    "    :return: Cluster labels for each data point.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract data from video\n",
    "    visual_features,frames=extractData(video_path, anno_file, info_file)\n",
    "\n",
    "    \n",
    "    \"\"\" \n",
    "    annotations,audio_features,visual_features,frames=extractData(video_path, anno_file, info_file)\n",
    "\n",
    "    # If annotations include categorical data, convert it to numerical format\n",
    "    # May exclude it later \n",
    "    label_encoder = LabelEncoder()\n",
    "    categorical_columns = ['category']\n",
    "    for column in categorical_columns:\n",
    "        if column in annotations.columns:\n",
    "            annotations[column] = label_encoder.fit_transform(annotations[column])\n",
    "\n",
    "    # Exclude non-numerical data from annotations if necessary\n",
    "    annotation_features = annotations.drop(columns=['video_id']).values\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # Determine the original feature length\n",
    "    original_feature_length = len(visual_features) + len(audio_features) + len(annotation_features)\n",
    "\n",
    "    # Combine features with padding\n",
    "    combined_features = []\n",
    "    \n",
    "    # Remove temporaraily the audio feature\n",
    "    for i, frame in enumerate(frames):\n",
    "        \n",
    "        annotation_float_array = annotation2List(annotation_features[i])\n",
    "        \n",
    "        visual_padded, audio_padded, annotation_padded = padding(visual_features, audio_features, annotation_float_array)\n",
    "\n",
    "        combined_feature = np.concatenate([\n",
    "            np.array(visual_padded[i], dtype=float),\n",
    "            # np.array(audio_padded, dtype=float),\n",
    "            np.array(annotation_padded[i], dtype=float),\n",
    "        ])\n",
    "        combined_features.append(combined_feature)\n",
    "\n",
    "    # Apply feature weighting\n",
    "    # weighted_combined_features = apply_feature_weighting(combined_features, original_feature_length)\n",
    "\n",
    "    # Convert to 2D NumPy array and normalize\n",
    "    # combined_features_array = np.array(weighted_combined_features)\n",
    "    \n",
    "    # combined_features_normalized = StandardScaler().fit_transform(combined_features_array)\n",
    "    # \"\"\"\n",
    "    \n",
    "    # Calculate the number of clusters if not provided\n",
    "    if num_clusters is None:\n",
    "        num_clusters = floor(sqrt(len(visual_features)))-5\n",
    "    \n",
    "    \n",
    "    # Perform clustering\n",
    "    print(\"Performing clustering...\")\n",
    "    kmeans = KMeans(n_clusters=7)\n",
    "    kmeans.fit(visual_features)\n",
    "    \n",
    "    return [frames, kmeans.labels_]\n",
    "    # return the frames and the labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find bigger cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def getBiggestCluster(labels):\n",
    "    cluster_counts = Counter(labels)\n",
    "\n",
    "    # Find the cluster with the maximum number of frames\n",
    "    return max(cluster_counts, key=cluster_counts.get),cluster_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frames Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_selection(frames, labels):\n",
    "    \"\"\"\n",
    "    Create a summary by selecting frames from the most populous cluster.\n",
    "\n",
    "    :param frames: List of frames.\n",
    "    :param labels: Cluster labels for each frame.\n",
    "    :return: List of frames belonging to the most populous cluster.\n",
    "    \"\"\"\n",
    "    # Count the number of frames in each cluster\n",
    "    max_cluster,cluster_counts = getBiggestCluster(labels)\n",
    "\n",
    "    # Initialize lists for indices and frames\n",
    "    summary_indices = []\n",
    "    summary_frames = []\n",
    "\n",
    "    # Iterate and select frames and their indices belonging to the most populous cluster\n",
    "    for index, (frame, label) in enumerate(zip(frames, labels)):\n",
    "        if label == max_cluster:\n",
    "            summary_indices.append(index)\n",
    "            summary_frames.append(frame)\n",
    "\n",
    "    print(f\"Number of frames selected for summary: {len(summary_frames)}, Cluster: {max_cluster}\")\n",
    "    # print all clusters len\n",
    "    print(f\"Cluster counts: {cluster_counts}\")\n",
    "    return summary_frames,summary_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video_from_frames(frames, output_path, frame_rate=25):\n",
    "    if not frames:\n",
    "        print(\"No frames to create a video.\")\n",
    "        return \n",
    "    # Determine the width and height from the first frame\n",
    "    height, width, layers = frames[0].shape\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, frame_rate, (width, height))\n",
    "\n",
    "    # Write each frame to the video\n",
    "    for frame in frames:\n",
    "        out.write(frame)\n",
    "\n",
    "    # Release the VideoWriter object\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load .Mat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_titles(encoded_titles, hdf5_file):\n",
    "    decoded_titles = []\n",
    "    for ref_array in encoded_titles:\n",
    "        # Handle the case where each ref_array might contain multiple references\n",
    "        for ref in ref_array:\n",
    "            # Dereference each HDF5 object reference to get the actual data\n",
    "            title_data = hdf5_file[ref]\n",
    "            # Decode the title\n",
    "            decoded_title = ''.join(chr(char[0]) for char in title_data)\n",
    "            decoded_titles.append(decoded_title)\n",
    "    return decoded_titles\n",
    "\n",
    "\n",
    "def load_mat_file(file_path,videoID):\n",
    "    \"\"\"\n",
    "    Load a .mat file and return its contents.\n",
    "\n",
    "    :param file_path: Path to the .mat file.\n",
    "    :return: Contents of the .mat file.\n",
    "    \"\"\"\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        user_anno_refs=file['tvsum50']['user_anno'][:] # type: ignore\n",
    "        video_refs=file['tvsum50']['video'][:] # type: ignore\n",
    "\n",
    "        decoded_videos = decode_titles(video_refs,file)\n",
    "    \n",
    "        annotations = []\n",
    "        \n",
    "        print(file['tvsum50'].keys())\n",
    "        \n",
    "        # Get the index from decoded video list to find the annotation for the video\n",
    "        index = [i for i, x in enumerate(decoded_videos) if x.lower() in videoID.lower()][0]\n",
    "        \n",
    "        # Iterate over each reference\n",
    "        for ref in user_anno_refs: # type: ignore\n",
    "            # Dereference each HDF5 object reference\n",
    "            ref_data = file[ref[0]]\n",
    "\n",
    "            # Convert to NumPy array and add to the annotations list\n",
    "            annotations.append(np.array(ref_data))\n",
    "            \n",
    "        return annotations[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f1score with ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_frame_selection(ground_truth, summary_indices):\n",
    "\n",
    "    # Evaluate the selected frames\n",
    "    selected_importance_scores = ground_truth[summary_indices]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    average_importance = np.mean(selected_importance_scores) # Average importance score\n",
    "    max_importance=np.max(selected_importance_scores) # Max importance score\n",
    "    # print('max:',max_importance) # 2.4\n",
    "    \n",
    "    proportion_high_importance = np.mean(selected_importance_scores >= floor(max_importance))  # Proportion of frames with max importance score\n",
    "    \n",
    "    return average_importance, max_importance,proportion_high_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_f1score(ground_truth_path, summary_indices,videoID):\n",
    "    ground_truth = np.array(load_mat_file(ground_truth_path, videoID))\n",
    "    n_annotators = ground_truth.shape[0]\n",
    "    \n",
    "    all_evaluations = []\n",
    "\n",
    "    for i in range(n_annotators):\n",
    "        individual_ground_truth_scores = ground_truth[i]\n",
    "\n",
    "\n",
    "        threshold=floor(individual_ground_truth_scores.mean())\n",
    "        # Binary conversion\n",
    "        binary_ground_truth = np.where(individual_ground_truth_scores >= threshold, 1, 0) \n",
    "\n",
    "        # Selected frames binary conversion\n",
    "        selected_frames_binary = np.zeros_like(binary_ground_truth)\n",
    "        selected_frames_binary[summary_indices] = 1\n",
    "\n",
    "        # Calculate metrics\n",
    "        precision = precision_score(binary_ground_truth, selected_frames_binary)\n",
    "        recall = recall_score(binary_ground_truth, selected_frames_binary)\n",
    "        f1 = f1_score(binary_ground_truth, selected_frames_binary, average='weighted')\n",
    "\n",
    "        # Importance-based evaluation\n",
    "        avg_importance, max_importance, prop_high_importance = evaluate_frame_selection(individual_ground_truth_scores, summary_indices)\n",
    "\n",
    "        all_evaluations.append({\n",
    "            \"annotator\": i,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"weighted_f1\": f1,\n",
    "            \"average_importance\": avg_importance,\n",
    "            \"maximum_importance\": max_importance,\n",
    "            \"proportion_high_importance\": prop_high_importance\n",
    "        })\n",
    "        \n",
    "    from tabulate import tabulate\n",
    "            \n",
    "    table_data = []\n",
    "    headers = [\"Annotator\", \"Precision\", \"Recall\", \"Weighted F1\", \"Avg Importance\", \"Max Importance\", \"Proportion High Importance\"]\n",
    "\n",
    "    for result in all_evaluations:\n",
    "        table_data.append([\n",
    "            result['annotator'],\n",
    "            f\"{result['precision']:.3f}\",\n",
    "            f\"{result['recall']:.3f}\",\n",
    "            f\"{result['weighted_f1']:.3f}\",\n",
    "            f\"{result['average_importance']:.3f}\",\n",
    "            f\"{result['maximum_importance']:.3f}\",\n",
    "            f\"{result['proportion_high_importance']:.3f}\"\n",
    "        ])\n",
    "\n",
    "    # Print the table\n",
    "    table_data.sort(key=lambda x: x[3],reverse=True)\n",
    "\n",
    "    print(tabulate(table_data, headers=headers, tablefmt=\"fancy_grid\"))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to create summary video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_path='datasets/ydata-tvsum50-v1_1/data/ydata-tvsum50-anno.tsv'\n",
    "info_path='datasets/ydata-tvsum50-v1_1/data/ydata-tvsum50-info.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path='datasets/ydata-tvsum50-v1_1/video/'\n",
    "summary_video_path='datasets/summary_videos/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_path='datasets/ydata-tvsum50-v1_1/ground_truth/ydata-tvsum50.mat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get the list of the videos in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_list = [video for video in os.listdir(video_path) if video.endswith('.mp4')]  # List comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for videoSummarizion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def videoSummarize(annotation_path=None, info_path=None, video_path=None, summary_video_path=None,video_list=None):\n",
    "    for video in video_list:  \n",
    "        frames,labels = integrate_and_cluster_features(video_path+video, annotation_path, info_path)\n",
    "        \n",
    "        # Select Representative Frames\n",
    "        summary_frames,summary_indices = frame_selection(frames, labels)  # Select frames from the most populous cluster\n",
    "        \n",
    "        # Create Summary Video\n",
    "        create_video_from_frames(summary_frames, summary_video_path+video,30) # Create a video from the selected frames\n",
    "        \n",
    "        # video: -esJrBWj2d8.mp4\n",
    "        kmeans_f1score(ground_truth_path,summary_frames,video.split('.')[0])\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for dynamic annotation and video summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoSummarize(annotation_path, info_path, video_path, summary_video_path, video_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results using 16 Clusters\n",
    "\n",
    "| Annotator | Precision | Recall | Weighted F1 | Avg Importance | Max Importance | Proportion High Importance |\n",
    "|-----------|-----------|--------|-------------|----------------|----------------|----------------------------|\n",
    "| 2         | 0.941     | 0.063  | 0.345       | 3.338          | 5              | 0.131                      |\n",
    "| 8         | 1.0       | 0.065  | 0.339       | 4.0            | 4              | 1.0                        |\n",
    "| 12        | 0.766     | 0.052  | 0.337       | 2.111          | 3              | 0.131                      |\n",
    "| 10        | 1.0       | 0.065  | 0.332       | 3.507          | 4              | 0.507                      |\n",
    "| 0         | 0.469     | 0.031  | 0.292       | 1.735          | 3              | 0.131                      |\n",
    "| 1         | 0.473     | 0.03   | 0.274       | 1.394          | 2              | 0.394                      |\n",
    "| 9         | 0.0       | 0.0    | 0.222       | 1.0            | 1              | 1.0                        |\n",
    "| 3         | 1.0       | 0.037  | 0.071       | 2.511          | 4              | 0.019                      |\n",
    "| 4         | 1.0       | 0.037  | 0.071       | 1.0            | 1              | 1.0                        |\n",
    "| 5         | 1.0       | 0.037  | 0.071       | 2.0            | 2              | 1.0                        |\n",
    "| 6         | 1.0       | 0.037  | 0.071       | 1.393          | 4              | 0.131                      |\n",
    "| 7         | 1.0       | 0.037  | 0.071       | 1.0            | 1              | 1.0                        |\n",
    "| 11        | 1.0       | 0.037  | 0.071       | 3.617          | 5              | 0.131                      |\n",
    "| 13        | 1.0       | 0.037  | 0.071       | 1.0            | 1              | 1.0                        |\n",
    "| 14        | 1.0       | 0.037  | 0.071       | 1.0            | 1              | 1.0                        |\n",
    "| 15        | 1.0       | 0.037  | 0.071       | 4.0            | 4              | 1.0                        |\n",
    "| 16        | 1.0       | 0.037  | 0.071       | 1.0            | 1              | 1.0                        |\n",
    "| 17        | 1.0       | 0.037  | 0.071       | 2.0            | 2              | 1.0                        |\n",
    "| 18        | 1.0       | 0.037  | 0.071       | 1.0            | 1              | 1.0                        |\n",
    "| 19        | 1.0       | 0.037  | 0.071       | 2.0            | 2              | 1.0                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results using 11 Clusters\n",
    "\n",
    "| Annotator | Precision | Recall | Weighted F1 | Avg Importance | Max Importance | Proportion High Importance |\n",
    "|-----------|-----------|--------|-------------|----------------|----------------|----------------------------|\n",
    "| 2         | 0.941     | 0.063  | 0.345       | 3.315          | 5              | 0.117                      |\n",
    "| 8         | 1.0       | 0.065  | 0.339       | 4.0            | 4              | 1.0                        |\n",
    "| 12        | 0.766     | 0.052  | 0.337       | 2.099          | 3              | 0.117                      |\n",
    "| 10        | 1.0       | 0.065  | 0.332       | 3.499          | 4              | 0.499                      |\n",
    "| 0         | 0.469     | 0.031  | 0.292       | 1.718          | 3              | 0.117                      |\n",
    "| 1         | 0.473     | 0.03   | 0.274       | 1.396          | 2              | 0.396                      |\n",
    "| 9         | 0.0       | 0.0    | 0.222       | 1.0            | 1              | 1.0                        |\n",
    "| 3         | 1.0       | 0.037  | 0.071       | 2.516          | 4              | 0.018                      |\n",
    "| 4         | 1.0       | 0.037  | 0.071       | 1.0            | 1              | 1.0                        |\n",
    "| 5         | 1.0       | 0.037  | 0.071       | 2.0            | 2              | 1.0                        |\n",
    "| 6         | 1.0       | 0.037  | 0.071       | 1.355          | 4              | 0.117                      |\n",
    "| 7         | 1.0       | 0.037  | 0.071       | 1.0            | 1              | 1.0                        |\n",
    "| 11        | 1.0       | 0.037  | 0.071       | 3.595          | 5              | 0.117                      |\n",
    "| 13        | 1.0       | 0.037  | 0.071       | 1.0            | 1              | 1.0                        |\n",
    "| 14        | 1.0       | 0.037  | 0.071       | 1.0            | 1              | 1.0                        |\n",
    "| 15        | 1.0       | 0.037  | 0.071       | 4.0            | 4              | 1.0                        |\n",
    "| 16        | 1.0       | 0.037  | 0.071       | 1.0            | 1              | 1.0                        |\n",
    "| 17        | 1.0       | 0.037  | 0.071       | 2.0            | 2              | 1.0                        |\n",
    "| 18        | 1.0       | 0.037  | 0.071       | 1.0            | 1              | 1.0                        |\n",
    "| 19        | 1.0       | 0.037  | 0.071       | 2.0            | 2              | 1.0                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results using 7 Clusters\n",
    "\n",
    "| Annotator | Precision | Recall | Weighted F1 | Avg Importance | Max Importance | Proportion High Importance |\n",
    "|-----------|-----------|--------|-------------|----------------|----------------|----------------------------|\n",
    "| 2         | 0.941     | 0.063  | 0.345       | 3.351          | 5              | 0.134                      |\n",
    "| 8         | 1.0       | 0.065  | 0.339       | 4.0            | 4              | 1.0                        |\n",
    "| 12        | 0.766     | 0.052  | 0.337       | 2.115          | 3              | 0.134                      |\n",
    "| 10        | 1.0       | 0.065  | 0.332       | 3.504          | 4              | 0.504                      |\n",
    "| 0         | 0.469     | 0.031  | 0.292       | 1.744          | 3              | 0.134                      |\n",
    "| 1         | 0.473     | 0.03   | 0.274       | 1.386          | 2              | 0.386                      |\n",
    "| 9         | 0.0       | 0.0    | 0.222       | 1.0            | 1              | 1.0                        |\n",
    "| 3         | 1.0       | 0.037  | 0.071       | 2.511          | 4              | 0.019                      |\n",
    "| 4         | 1.0       | 0.037  | 0.071       | 1.0            | 1              | 1.0                        |\n",
    "| 5         | 1.0       | 0.037  | 0.071       | 2.0            | 2              | 1.0                        |\n",
    "| 6         | 1.0       | 0.037  | 0.071       | 1.405          | 4              | 0.134                      |\n",
    "| 7         | 1.0       | 0.037  | 0.071       | 1.0            | 1              | 1.0                        |\n",
    "| 11        | 1.0       | 0.037  | 0.071       | 3.615          | 5              | 0.134                      |\n",
    "| 13        | 1.0       | 0.037  | 0.071       | 1.0            | 1              | 1.0                        |\n",
    "| 14        | 1.0       | 0.037  | 0.071       | 1.0            | 1              | 1.0                        |\n",
    "| 15        | 1.0       | 0.037  | 0.071       | 4.0            | 4              | 1.0                        |\n",
    "| 16        | 1.0       | 0.037  | 0.071       | 1.0            | 1              | 1.0                        |\n",
    "| 17        | 1.0       | 0.037  | 0.071       | 2.0            | 2              | 1.0                        |\n",
    "| 18        | 1.0       | 0.037  | 0.071       | 1.0            | 1              | 1.0                        |\n",
    "| 19        | 1.0       | 0.037  | 0.071       | 2.0            | 2              | 1.0                        |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
