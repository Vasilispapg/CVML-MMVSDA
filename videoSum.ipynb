{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from sklearn.cluster import KMeans\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from math import floor,sqrt\n",
    "from moviepy.editor import VideoFileClip\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import h5py\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import objectDetection as od\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotations loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations\n",
    "def load_annotations(anno_file, info_file):\n",
    "    # Read the annotation and info files\n",
    "    annotations = pd.read_csv(anno_file, sep='\\t', header=None)\n",
    "    info = pd.read_csv(info_file, sep='\\t', header=None)\n",
    "    \n",
    "    # Rename columns for better understanding\n",
    "    annotations.columns = ['video_id', 'category', 'importance_score']\n",
    "    info.columns = ['category_code', 'video_id', 'title', 'url', 'length']\n",
    "    \n",
    "    # print( annotations, info)\n",
    "    \n",
    "    return annotations, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Feature Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Video Processing and Frame Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, frame_rate=1):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    count = 0\n",
    "    success = True\n",
    "    frames = []\n",
    "    \n",
    "    while success:\n",
    "        success, image = video.read()\n",
    "        if count % frame_rate == 0 and success:\n",
    "            frames.append(image)\n",
    "        count += 1\n",
    "\n",
    "    video.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Audio Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_audio_from_video(video_path, output_audio_path):\n",
    "    video = VideoFileClip(video_path)\n",
    "    audio = video.audio\n",
    "    audio.write_audiofile(output_audio_path)\n",
    "    video.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c.Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features_for_each_frame(audio_path, frame_rate):\n",
    "    y, sr = librosa.load(audio_path)\n",
    "\n",
    "    # Calculate the number of audio samples per video frame\n",
    "    samples_per_frame = sr / frame_rate\n",
    "\n",
    "    # Initialize an array to store MFCCs for each frame\n",
    "    mfccs_per_frame = []\n",
    "\n",
    "    # Iterate over each frame and extract corresponding MFCCs\n",
    "    for frame in range(int(len(y) / samples_per_frame)):\n",
    "        start_sample = int(frame * samples_per_frame)\n",
    "        end_sample = int((frame + 1) * samples_per_frame)\n",
    "\n",
    "        # Ensure the end sample does not exceed the audio length\n",
    "        end_sample = min(end_sample, len(y))\n",
    "\n",
    "        # Extract MFCCs for the current frame's audio segment\n",
    "        mfccs_current_frame = librosa.feature.mfcc(y=y[start_sample:end_sample], sr=sr, n_mfcc=13)\n",
    "        mfccs_processed = np.mean(mfccs_current_frame.T, axis=0)\n",
    "        mfccs_per_frame.append(mfccs_processed)\n",
    "    print(np.array(mfccs_per_frame).shape)\n",
    "\n",
    "    return mfccs_per_frame[:len(mfccs_per_frame)-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Feature Extraction (example with visual features using a CNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "def extract_visual_features(frames):\n",
    "    features = []\n",
    "    for frame in frames:\n",
    "        if frame is not None:\n",
    "            img = cv2.resize(frame, (224, 224))  # Resize frame to 224x224\n",
    "            img = img_to_array(img)        # Convert to array\n",
    "            img = np.expand_dims(img, axis=0)    # Add batch dimension\n",
    "            img = preprocess_input(img)          # Preprocess for VGG16\n",
    "            \n",
    "            feature = model.predict(img,use_multiprocessing=True,workers=4)\n",
    "            features.append(feature.flatten())\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect Audio/Annotation/Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotation To List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation2List(annotation_features):\n",
    "    # Make the string '1,1,1,3,2,2,4,4,1' to float list\n",
    "    annotation_float_array=[]\n",
    "    for annotation in annotation_features:\n",
    "        \n",
    "        if isinstance(annotation,str):\n",
    "            annotation = annotation.split(',')\n",
    "        if isinstance(annotation,list):\n",
    "            for anno in annotation:\n",
    "                annotation_float_array.append(float(anno))\n",
    "        else:\n",
    "            annotation_float_array.append(float(annotation))\n",
    "    return annotation_float_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('objects.pkl', 'rb') as f:\n",
    "    objects = pickle.load(f)\n",
    "\n",
    "with open('frames.pkl', 'rb') as f:\n",
    "    frames = pickle.load(f)\n",
    "\n",
    "with open('labels.pkl', 'rb') as f:\n",
    "    labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractData(video_path, anno_file, info_file):\n",
    "    # Extract frames from the video\n",
    "    frames = extract_frames(video_path)\n",
    "    # print('frames',len(frames)) \n",
    "\n",
    "    # # Extract visual features\n",
    "    visual_features = extract_visual_features(frames) #type: ignore\n",
    "\n",
    "    # Extract audio\n",
    "    audio_output_path = 'datasets/extractedAudio/extracted_audio.wav'\n",
    "    extract_audio_from_video(video_path, audio_output_path) \n",
    "\n",
    "    # Extract audio features\n",
    "    audio_features = extract_audio_features_for_each_frame(audio_output_path,30)\n",
    "\n",
    "    # # Load annotations\n",
    "    # annotations, info = load_annotations(anno_file, info_file)\n",
    "    \n",
    "    return visual_features, frames,audio_features#type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kmeans and feature connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(objects_in_frame, unique_objects):\n",
    "    \"\"\"\n",
    "    Convert a list of objects detected in a frame to a one-hot encoded vector.\n",
    "    \n",
    "    Args:\n",
    "    objects_in_frame: List of objects detected in a frame.\n",
    "    unique_objects: List of all unique objects across all frames.\n",
    "\n",
    "    Returns:\n",
    "    One-hot encoded vector representing the presence of objects in the frame.\n",
    "    \"\"\"\n",
    "    encoding = [1 if obj in objects_in_frame else 0 for obj in unique_objects]\n",
    "    return np.array(encoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiModalKMeans(video_path, anno_file, info_file, num_clusters=None):\n",
    "    \"\"\"\n",
    "    Integrate visual, audio, and annotation features from a video,\n",
    "    and perform clustering on the combined features.\n",
    "\n",
    "    :param video_path: Path to the video file.\n",
    "    :param anno_file: Path to the annotation file.\n",
    "    :param info_file: Path to the info file.\n",
    "    :param num_clusters: Number of clusters to use in KMeans.\n",
    "    :return: Cluster labels for each data point.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract data from video\n",
    "    visual_features,frames,audio_features=extractData(video_path, anno_file, info_file)\n",
    "        \n",
    "    objects = od.detect_objects_in_all_frames(frames,yolo_model,classes) #type: ignore\n",
    "            \n",
    "    unique_objects = sorted(list(set([obj for frame_objects in objects for obj in frame_objects]))) #type: ignore\n",
    "\n",
    "    encoded_objects = [one_hot_encode(frame_objects, unique_objects) for frame_objects in objects] #type: ignore\n",
    "\n",
    "    # Combine features with padding\n",
    "    combined_features = []\n",
    "\n",
    "    for i, frame in enumerate(frames):\n",
    "        \n",
    "        # annotation_float_array = annotation2List(annotation_features[i])\n",
    "        \n",
    "        combined_feature = np.concatenate([\n",
    "            np.array(visual_features[i], dtype=float),\n",
    "            np.array(audio_features[i], dtype=float),\n",
    "            np.array(encoded_objects[i], dtype=float),\n",
    "        ])\n",
    "        combined_features.append(combined_feature)\n",
    "\n",
    "    # Convert to 2D NumPy array and normalize\n",
    "    combined_features_array = np.array(combined_features)\n",
    "    \n",
    "    combined_features_normalized = StandardScaler().fit_transform(combined_features_array)\n",
    "         \n",
    "    print(\"Number of clusters:\", num_clusters)\n",
    "    \n",
    "    # Perform clustering\n",
    "    print(\"Performing clustering...\")\n",
    "    kmeans = KMeans(n_clusters=num_clusters) #type: ignore\n",
    "    kmeans.fit(combined_features_normalized)\n",
    "    \n",
    "    return [frames, kmeans.labels_]\n",
    "    # return the frames and the labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find min cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def getMinClusster(labels):\n",
    "    cluster_counts = Counter(labels)\n",
    "\n",
    "    # Find the cluster with the maximum number of frames\n",
    "    return min(cluster_counts, key=cluster_counts.get),cluster_counts #type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frames Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_selection(frames, labels):\n",
    "    \"\"\"\n",
    "    Create a summary by selecting frames from the most populous cluster.\n",
    "\n",
    "    :param frames: List of frames.\n",
    "    :param labels: Cluster labels for each frame.\n",
    "    :return: List of frames belonging to the most populous cluster.\n",
    "    \"\"\"\n",
    "    # Count the number of frames in each cluster\n",
    "    max_cluster,cluster_counts = getMinClusster(labels)\n",
    "\n",
    "    # Initialize lists for indices and frames\n",
    "    summary_indices = []  # to measure the importance based on annotation\n",
    "    summary_frames = []\n",
    "\n",
    "    # Iterate and select frames and their indices belonging to the most populous cluster\n",
    "    for index, (frame, label) in enumerate(zip(frames, labels)):\n",
    "        if label == max_cluster:\n",
    "            summary_indices.append(index)\n",
    "            summary_frames.append(frame)\n",
    "\n",
    "    print(f\"Number of frames selected for summary: {len(summary_frames)}, Cluster: {max_cluster}\")\n",
    "    # print all clusters len\n",
    "    print(f\"Cluster counts: {cluster_counts}\")\n",
    "    return summary_frames,summary_indices\n",
    "\n",
    "def frameSelectionForEachCluster(frames):\n",
    "    \"\"\"\n",
    "    Create a summary by selecting frames from the most populous cluster.\n",
    "\n",
    "    :param frames: List of frames.\n",
    "    :param labels: Cluster labels for each frame.\n",
    "    :return: List of frames belonging to the most populous cluster.\n",
    "    \"\"\"\n",
    "    # Initialize lists for indices and frames\n",
    "    summary_indices = []  # to measure the importance based on annotation\n",
    "    summary_frames = []\n",
    "\n",
    "    # Iterate and select frames and their indices belonging to the most populous cluster\n",
    "    for index, (frame) in enumerate(frames):\n",
    "        summary_indices.append(index)\n",
    "        summary_frames.append(frame)\n",
    "\n",
    "    print(f\"Number of frames selected for summary: {len(summary_frames)}\")\n",
    "    # print all clusters len\n",
    "    # print(f\"Cluster counts: {cluster_counts}\")\n",
    "    return summary_frames,summary_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video_from_frames(frames, output_path, frame_rate=30):\n",
    "    if not frames:\n",
    "        print(\"No frames to create a video.\")\n",
    "        return \n",
    "    # Determine the width and height from the first frame\n",
    "    height, width, layers = frames[0].shape\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')#type: ignore\n",
    "    out = cv2.VideoWriter(output_path, fourcc, frame_rate, (width, height))#type: ignore\n",
    "\n",
    "    # Write each frame to the video\n",
    "    for frame in frames:\n",
    "        out.write(frame)\n",
    "\n",
    "    # Release the VideoWriter object\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load .Mat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_titles(encoded_titles, hdf5_file):\n",
    "    decoded_titles = []\n",
    "    for ref_array in encoded_titles:\n",
    "        # Handle the case where each ref_array might contain multiple references\n",
    "        for ref in ref_array:\n",
    "            # Dereference each HDF5 object reference to get the actual data\n",
    "            title_data = hdf5_file[ref]\n",
    "            # Decode the title\n",
    "            decoded_title = ''.join(chr(char[0]) for char in title_data)\n",
    "            decoded_titles.append(decoded_title)\n",
    "    return decoded_titles\n",
    "\n",
    "\n",
    "def load_mat_file(file_path,videoID):\n",
    "    \"\"\"\n",
    "    Load a .mat file and return its contents.\n",
    "\n",
    "    :param file_path: Path to the .mat file.\n",
    "    :return: Contents of the .mat file.\n",
    "    \"\"\"\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        user_anno_refs=file['tvsum50']['user_anno'][:] # type: ignore\n",
    "        video_refs=file['tvsum50']['video'][:] # type: ignore\n",
    "\n",
    "        decoded_videos = decode_titles(video_refs,file)\n",
    "    \n",
    "        annotations = []        \n",
    "        # Get the index from decoded video list to find the annotation for the video\n",
    "        index = [i for i, x in enumerate(decoded_videos) if x.lower() in videoID.lower()][0]\n",
    "        \n",
    "        # Iterate over each reference\n",
    "        for ref in user_anno_refs: # type: ignore\n",
    "            # Dereference each HDF5 object reference\n",
    "            ref_data = file[ref[0]]\n",
    "\n",
    "            # Convert to NumPy array and add to the annotations list\n",
    "            annotations.append(np.array(ref_data))\n",
    "            \n",
    "        return annotations[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f1score with ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_frame_selection(ground_truth, summary_indices):\n",
    "    \"\"\"\n",
    "    Evaluate the selected frames by comparing them with the ground truth.\n",
    "    \n",
    "    Args:\n",
    "    ground_truth: Ground truth annotations.\n",
    "    summary_indices: Indices of the selected frames.\n",
    "    \n",
    "    Returns:\n",
    "    Average importance score, max importance score, and proportion of frames with high importance score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Evaluate the selected frames\n",
    "    selected_importance_scores = ground_truth[summary_indices]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if selected_importance_scores.size == 0:\n",
    "        average_importance = 0\n",
    "        max_importance = 0\n",
    "        proportion_high_importance = 0\n",
    "    else:\n",
    "        average_importance = np.mean(selected_importance_scores)  # Average importance score\n",
    "        max_importance = np.max(selected_importance_scores)  # Max importance score\n",
    "        # Calculate the proportion of frames with high importance score\n",
    "        proportion_high_importance = np.mean(selected_importance_scores >= np.floor(max_importance))\n",
    "        \n",
    "    return average_importance, max_importance,proportion_high_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluation(ground_truth_path,summary_indices,videoID):\n",
    "    \n",
    "    # Get the ground_truth\n",
    "    ground_truth = np.array(load_mat_file(ground_truth_path, videoID))\n",
    "    \n",
    "    # Find the mean of all annotators\n",
    "    gold_standard = np.mean(ground_truth, axis=0)\n",
    "    \n",
    "    # based on annotators find the avg_importance, max_importance, prop_high_importance\n",
    "    avg_importance, max_importance, prop_high_importance = evaluate_frame_selection(gold_standard, summary_indices)\n",
    "\n",
    "    # Thresholding based on max_importance\n",
    "    threshold=floor(np.mean(avg_importance))\n",
    "    \n",
    "    \n",
    "    # Binary conversion\n",
    "    binary_ground_truth = np.where(gold_standard >= threshold, 1, 0) \n",
    "    \n",
    "    # Selected frames binary conversion\n",
    "    selected_frames_binary = np.zeros_like(binary_ground_truth)\n",
    "    selected_frames_binary[summary_indices] = 1\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = precision_score(binary_ground_truth, selected_frames_binary)\n",
    "    recall = recall_score(binary_ground_truth, selected_frames_binary)  \n",
    "    f1 = f1_score(binary_ground_truth, selected_frames_binary, average='macro')\n",
    "    \n",
    "    # return metrics\n",
    "    return threshold,precision,recall,f1,avg_importance,max_importance,prop_high_importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KnapSack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knapsack_for_video_summary(values, weights, capacity, scale_factor=30):\n",
    "    \"\"\"\n",
    "    Apply the 0/1 Knapsack algorithm to select video segments for summarization.\n",
    "\n",
    "    :param values: List of importance scores for each segment.\n",
    "    :param weights: List of durations for each segment in seconds.\n",
    "    :param capacity: Maximum total duration for the summary in seconds.\n",
    "    :param scale_factor: Factor to scale weights to integers.\n",
    "    :return: Indices of the segments to include in the summary.\n",
    "    \"\"\"\n",
    "    # Scale weights and capacity\n",
    "    weights = [int(w * scale_factor) for w in weights]\n",
    "    capacity = int(capacity * scale_factor)\n",
    "\n",
    "    n = len(values)\n",
    "    K = [[0 for _ in range(capacity + 1)] for _ in range(n + 1)]\n",
    "\n",
    "    # Build table K[][] in a bottom-up manner\n",
    "    for i in range(n + 1):\n",
    "        for w in range(capacity + 1):\n",
    "            if i == 0 or w == 0:\n",
    "                K[i][w] = 0\n",
    "            elif weights[i-1] <= w:\n",
    "                K[i][w] = max(values[i-1] + K[i-1][w-weights[i-1]], K[i-1][w])\n",
    "            else:\n",
    "                K[i][w] = K[i-1][w]\n",
    "\n",
    "    # Find the selected segments\n",
    "    res = K[n][capacity]\n",
    "    w = capacity\n",
    "    selected_indices = []\n",
    "\n",
    "    for i in range(n, 0, -1):\n",
    "        if res <= 0:\n",
    "            break\n",
    "        if res == K[i-1][w]:\n",
    "            continue\n",
    "        else:\n",
    "            selected_indices.append(i-1)\n",
    "            res = res - values[i-1]\n",
    "            w = w - weights[i-1]\n",
    "\n",
    "    selected_indices.reverse()\n",
    "    return selected_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importance score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_importance_scores(labels, detected_objects):\n",
    "    \"\"\"Here the importance for all the labels together..\n",
    "\n",
    "    Args:\n",
    "        labels (_type_): _description_\n",
    "        detected_objects (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # Count the frequency of each label\n",
    "    label_counts = Counter(labels)\n",
    "\n",
    "    # Assign a basic importance score based on the inverse frequency of cluster labels\n",
    "    base_importance_scores = [1 / label_counts[label] for label in labels]\n",
    "\n",
    "    # Additional importance based on detected objects\n",
    "    object_importance_scores = []\n",
    "    for objects in detected_objects:\n",
    "        if objects:  # if the list is not empty\n",
    "            # Add extra importance for each detected object\n",
    "            # You can customize this part based on the type and number of objects\n",
    "            object_importance_scores.append(len(objects))\n",
    "        else:\n",
    "            object_importance_scores.append(0)\n",
    "\n",
    "    # Combine base importance scores with object importance scores\n",
    "    # Normalize object importance scores for simplicity\n",
    "    max_object_score = max(object_importance_scores) if object_importance_scores else 1\n",
    "    normalized_object_scores = [score / max_object_score for score in object_importance_scores]\n",
    "\n",
    "    # Final importance score is a combination of base and object scores\n",
    "    importance_scores = [base + obj for base, obj in zip(base_importance_scores, normalized_object_scores)]\n",
    "    \n",
    "    print(\"Average importance score:\", np.mean(importance_scores))\n",
    "    \n",
    "    return importance_scores\n",
    "\n",
    "\n",
    "def calculate_importance_scores_for_cluster(cluster_frames, detected_objects_per_frame):\n",
    "    \"\"\"Here the importance for each frame based on objects in frame\n",
    "\n",
    "    Args:\n",
    "        cluster_dict (_type_): _description_\n",
    "        detected_objects_per_frame (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    # Initialize importance scores for the cluster\n",
    "    importance_scores = []\n",
    "\n",
    "    # Calculate object-based importance for each frame in the cluster\n",
    "    for frame_index in cluster_frames:\n",
    "        # Calculate object importance\n",
    "        objects = detected_objects_per_frame[frame_index]\n",
    "        object_importance = len(objects) if objects else 0\n",
    "\n",
    "        # Add object importance to the list\n",
    "        importance_scores.append(object_importance)\n",
    "\n",
    "    # Normalize the importance scores\n",
    "    max_importance = max(importance_scores, default=1)\n",
    "    if max_importance == 0:\n",
    "        max_importance = 1  # To prevent division by zero\n",
    "\n",
    "    normalized_importance_scores = [score / max_importance for score in importance_scores]\n",
    "\n",
    "    return normalized_importance_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map labels with frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_frames_to_labels_with_indices(frames, labels):\n",
    "    label_frame_dict = {}\n",
    "    for label, (frame_index, frame) in zip(labels, enumerate(frames)):\n",
    "        if label not in label_frame_dict:\n",
    "            label_frame_dict[label] = []\n",
    "        label_frame_dict[label].append((frame_index, frame))\n",
    "    return label_frame_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to create summary video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_path='datasets/ydata-tvsum50-v1_1/data/ydata-tvsum50-anno.tsv'\n",
    "info_path='datasets/ydata-tvsum50-v1_1/data/ydata-tvsum50-info.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path='datasets/ydata-tvsum50-v1_1/video/'\n",
    "summary_video_path='datasets/summary_videos/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_path='datasets/ydata-tvsum50-v1_1/ground_truth/ydata-tvsum50.mat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get the list of the videos in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_list = [video for video in os.listdir(video_path) if video.endswith('.mp4')]  # List comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Yolo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model = cv2.dnn.readNetFromDarknet('yolo/yolov3.cfg', 'yolo/yolov3.weights') #type:ignore\n",
    "\n",
    "with open(\"yolo/coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_rate = 30  # or whatever your frame rate is\n",
    "frame_duration = 1 / frame_rate  # Duration of each frame in seconds\n",
    "capacity = 15  # 15 seconds summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for videoSummarizion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def videoSumm(annotation_path=None, info_path=None, video_path=None, summary_video_path=None,video_list=None):\n",
    "    for video in video_list: #type: ignore\n",
    "        \n",
    "        # Kmeans clustering\n",
    "        # frames,labels = MultiModalKMeans(video_path+video, annotation_path, info_path,num_clusters=11)\n",
    "\n",
    "        \"\"\" USE THIS IN CASE OF CREATING VIDEO FOR EACH CLUSTER\"\"\"\n",
    "        cluster_dict = map_frames_to_labels_with_indices(frames, labels)\n",
    "\n",
    "        scores = []\n",
    "        for index, cluster in cluster_dict.items():\n",
    "            print(\"Cluster:\", index)\n",
    "            # Extract just the frame indices for importance score calculation\n",
    "            cluster_frame_indices = [frame_index for frame_index, _ in cluster]\n",
    "\n",
    "            # Calculate importance scores for this cluster\n",
    "            importance = calculate_importance_scores_for_cluster(cluster_frame_indices, objects) # Adjust your function as necessary\n",
    "\n",
    "            # Create a list of durations, one for each frame in the cluster\n",
    "            durations = [frame_duration] * len(importance)\n",
    "\n",
    "            # Apply the knapsack algorithm\n",
    "            summary_indices = knapsack_for_video_summary(importance, durations, capacity)\n",
    "\n",
    "            # Select Representative Frames (extracting frame part from each tuple)\n",
    "            summary_frames = [cluster[i][1] for i in summary_indices]  # [1] extracts the frame from (frame_index, frame)\n",
    "\n",
    "            # Create Summary Video\n",
    "            create_video_from_frames(summary_frames, f\"{summary_video_path}{index}-{video}\", 30)\n",
    "\n",
    "            # Extract original indices for evaluation\n",
    "            original_indices_for_eval = [cluster[i][0] for i in summary_indices]  # [0] extracts the original frame index\n",
    "\n",
    "            # Evaluate\n",
    "            ev = Evaluation(ground_truth_path, original_indices_for_eval, video.split('.')[0])\n",
    "            scores.append((str(index),) + ev)\n",
    "            print()\n",
    "        # Print the table of scores\n",
    "        from tabulate import tabulate\n",
    "        scores.sort(key=lambda x: x[3], reverse=True)\n",
    "        print(tabulate(scores, headers=['videoID','Threshold','Precision', 'Recall', 'F1 Score', 'Avg_importance', 'Max_importance', 'Prop_high_importance'], tablefmt='fancy_grid'))\n",
    "        return\n",
    "    \n",
    "        \"\"\"\n",
    "        USE THIS IN CASE OF ONE CLUSTER\n",
    "        importance=calculate_importance_scores(labels,objects) #type: ignore\n",
    "                \n",
    "        # Create a list of durations, one for each frame\n",
    "        durations = [frame_duration] * len(importance)\n",
    "        \n",
    "        # Apply the knapsack algorithm\n",
    "        summary_indices = knapsack_for_video_summary(importance, durations, capacity)\n",
    "\n",
    "        # Extract summary frames based on 'summary_indices'\n",
    "        summary_frames = [frames[i] for i in summary_indices] #type: ignore\n",
    "        \n",
    "        create_video_from_frames(summary_frames, summary_video_path+video,30) # Create a video from the selected frames\n",
    "        \n",
    "        Evaluation(ground_truth_path,summary_indices,video.split('.')[0])\n",
    "        break\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for object detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for dynamic annotation and video summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 6\n",
      "\n",
      "Cluster: 2\n",
      "\n",
      "Cluster: 9\n",
      "\n",
      "Cluster: 8\n",
      "\n",
      "Cluster: 10\n",
      "\n",
      "Cluster: 3\n",
      "No frames to create a video.\n",
      "\n",
      "Cluster: 7\n",
      "\n",
      "Cluster: 5\n",
      "\n",
      "Cluster: 1\n",
      "\n",
      "Cluster: 4\n",
      "\n",
      "Cluster: 0\n",
      "\n",
      "╒═══════════╤═════════════╤═════════════╤═══════════╤════════════╤══════════════════╤══════════════════╤════════════════════════╕\n",
      "│   videoID │   Threshold │   Precision │    Recall │   F1 Score │   Avg_importance │   Max_importance │   Prop_high_importance │\n",
      "╞═══════════╪═════════════╪═════════════╪═══════════╪════════════╪══════════════════╪══════════════════╪════════════════════════╡\n",
      "│         8 │           3 │    0.821229 │ 0.7       │  0.87061   │          3.06131 │             3.35 │             0.821229   │\n",
      "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
      "│         4 │           2 │    1        │ 0.0971698 │  0.449667  │          2.13625 │             2.4  │             1          │\n",
      "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
      "│         2 │           2 │    0.984733 │ 0.0811321 │  0.434039  │          2.61374 │             3    │             0.167939   │\n",
      "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
      "│         6 │           1 │    1        │ 0.0651042 │  0.0611247 │          1.92333 │             2.4  │             0.437778   │\n",
      "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
      "│         5 │           1 │    1        │ 0.0651042 │  0.0611247 │          1.626   │             2    │             0.12       │\n",
      "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
      "│         1 │           1 │    1        │ 0.0651042 │  0.0611247 │          1.44733 │             2.15 │             0.00888889 │\n",
      "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
      "│         9 │           2 │    1        │ 0.0327044 │  0.385748  │          2.6024  │             2.7  │             1          │\n",
      "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
      "│         0 │           1 │    1        │ 0.0231481 │  0.0226244 │          1.48125 │             1.5  │             1          │\n",
      "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
      "│         7 │           1 │    1        │ 0.0157697 │  0.0155249 │          1.5     │             1.65 │             1          │\n",
      "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
      "│        10 │           1 │    1        │ 0.0120081 │  0.0118656 │          1.93675 │             2.45 │             0.192771   │\n",
      "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
      "│         3 │           0 │    0        │ 0         │  0         │          0       │             0    │             0          │\n",
      "╘═══════════╧═════════════╧═════════════╧═══════════╧════════════╧══════════════════╧══════════════════╧════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "videoSumm(annotation_path, info_path, video_path, summary_video_path, video_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results after the video call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '╒' (U+2552) (3961589821.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[29], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    ╒═══════════╤═════════════╤═════════════╤════════════╤════════════╤══════════════════╤══════════════════╤════════════════════════╕\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character '╒' (U+2552)\n"
     ]
    }
   ],
   "source": [
    "╒═══════════╤═════════════╤═════════════╤════════════╤════════════╤══════════════════╤══════════════════╤════════════════════════╕\n",
    "│   videoID │   Threshold │   Precision │     Recall │  F1 BINARY │   Avg_importance │   Max_importance │   Prop_high_importance │\n",
    "╞═══════════╪═════════════╪═════════════╪════════════╪════════════╪══════════════════╪══════════════════╪════════════════════════╡\n",
    "│         8 │           3 │  0.821229   │ 0.7        │ 0.755784   │          3.06131 │             3.35 │             0.821229   │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         2 │           3 │  0.167939   │ 0.104762   │ 0.129032   │          2.61374 │             3    │             0.167939   │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         4 │           2 │  1          │ 0.0971698  │ 0.177128   │          2.13625 │             2.4  │             1          │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         6 │           2 │  0.437778   │ 0.0619497  │ 0.10854    │          1.92333 │             2.4  │             0.437778   │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         9 │           2 │  1          │ 0.0327044  │ 0.0633374  │          2.6024  │             2.7  │             1          │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         0 │           1 │  1          │ 0.0231481  │ 0.0452489  │          1.48125 │             1.5  │             1          │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         5 │           2 │  0.12       │ 0.0169811  │ 0.0297521  │          1.626   │             2    │             0.12       │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         7 │           1 │  1          │ 0.0157697  │ 0.0310497  │          1.5     │             1.65 │             1          │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│        10 │           2 │  0.192771   │ 0.00503145 │ 0.00980693 │          1.93675 │             2.45 │             0.192771   │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         1 │           2 │  0.00888889 │ 0.00125786 │ 0.00220386 │          1.44733 │             2.15 │             0.00888889 │\n",
    "├───────────┼─────────────┼─────────────┼────────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         3 │           0 │  0          │ 0          │ 0          │          0       │             0    │             0          │\n",
    "╘═══════════╧═════════════╧═════════════╧════════════╧════════════╧══════════════════╧══════════════════╧════════════════════════╛\n",
    "\n",
    "╒═══════════╤═════════════╤═════════════╤═══════════╤═══════════╤══════════════════╤══════════════════╤════════════════════════╕\n",
    "│   videoID │   Threshold │   Precision │    Recall │ F1 BINARY │   Avg_importance │   Max_importance │   Prop_high_importance │\n",
    "╞═══════════╪═════════════╪═════════════╪═══════════╪═══════════╪══════════════════╪══════════════════╪════════════════════════╡\n",
    "│         8 │           3 │    0.821229 │ 0.7       │ 0.755784  │          3.06131 │             3.35 │             0.821229   │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼───────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         4 │           2 │    1        │ 0.0971698 │ 0.177128  │          2.13625 │             2.4  │             1          │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼───────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         2 │           2 │    0.984733 │ 0.0811321 │ 0.149913  │          2.61374 │             3    │             0.167939   │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼───────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         6 │           1 │    1        │ 0.0651042 │ 0.122249  │          1.92333 │             2.4  │             0.437778   │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼───────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         5 │           1 │    1        │ 0.0651042 │ 0.122249  │          1.626   │             2    │             0.12       │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼───────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         1 │           1 │    1        │ 0.0651042 │ 0.122249  │          1.44733 │             2.15 │             0.00888889 │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼───────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         9 │           2 │    1        │ 0.0327044 │ 0.0633374 │          2.6024  │             2.7  │             1          │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼───────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         0 │           1 │    1        │ 0.0231481 │ 0.0452489 │          1.48125 │             1.5  │             1          │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼───────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         7 │           1 │    1        │ 0.0157697 │ 0.0310497 │          1.5     │             1.65 │             1          │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼───────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│        10 │           1 │    1        │ 0.0120081 │ 0.0237312 │          1.93675 │             2.45 │             0.192771   │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼───────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         3 │           0 │    0        │ 0         │ 0         │          0       │             0    │             0          │\n",
    "╘═══════════╧═════════════╧═════════════╧═══════════╧═══════════╧══════════════════╧══════════════════╧════════════════════════╛\n",
    "\n",
    "╒═══════════╤═════════════╤═════════════╤═══════════╤════════════╤══════════════════╤══════════════════╤════════════════════════╕\n",
    "│   videoID │   Threshold │   Precision │    Recall │   F1 MACRO │   Avg_importance │   Max_importance │   Prop_high_importance │\n",
    "╞═══════════╪═════════════╪═════════════╪═══════════╪════════════╪══════════════════╪══════════════════╪════════════════════════╡\n",
    "│         8 │           3 │    0.821229 │ 0.7       │  0.87061   │          3.06131 │             3.35 │             0.821229   │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         4 │           2 │    1        │ 0.0971698 │  0.449667  │          2.13625 │             2.4  │             1          │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         2 │           2 │    0.984733 │ 0.0811321 │  0.434039  │          2.61374 │             3    │             0.167939   │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         6 │           1 │    1        │ 0.0651042 │  0.0611247 │          1.92333 │             2.4  │             0.437778   │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         5 │           1 │    1        │ 0.0651042 │  0.0611247 │          1.626   │             2    │             0.12       │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         1 │           1 │    1        │ 0.0651042 │  0.0611247 │          1.44733 │             2.15 │             0.00888889 │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         9 │           2 │    1        │ 0.0327044 │  0.385748  │          2.6024  │             2.7  │             1          │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         0 │           1 │    1        │ 0.0231481 │  0.0226244 │          1.48125 │             1.5  │             1          │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         7 │           1 │    1        │ 0.0157697 │  0.0155249 │          1.5     │             1.65 │             1          │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│        10 │           1 │    1        │ 0.0120081 │  0.0118656 │          1.93675 │             2.45 │             0.192771   │\n",
    "├───────────┼─────────────┼─────────────┼───────────┼────────────┼──────────────────┼──────────────────┼────────────────────────┤\n",
    "│         3 │           0 │    0        │ 0         │  0         │          0       │             0    │             0          │\n",
    "╘═══════════╧═════════════╧═════════════╧═══════════╧════════════╧══════════════════╧══════════════════╧════════════════════════╛"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
