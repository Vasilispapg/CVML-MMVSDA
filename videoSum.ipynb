{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "import cv2\n",
    "from sklearn.cluster import KMeans\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from math import floor,sqrt,ceil,log2\n",
    "from moviepy.editor import VideoFileClip\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import h5py\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import objectDetection as od\n",
    "from tabulate import tabulate\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import isodata\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Feature Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Video Processing and Frame Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, frame_rate=2):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    count = 0\n",
    "    success = True\n",
    "    frames = []\n",
    "    \n",
    "    while success:\n",
    "        success, image = video.read()\n",
    "        if count % frame_rate == 0 and success:\n",
    "            frames.append(image)\n",
    "        count += 1\n",
    "\n",
    "    video.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Audio Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_audio_from_video(video_path, output_audio_path):\n",
    "    video = VideoFileClip(video_path)\n",
    "    audio = video.audio\n",
    "    audio.write_audiofile(output_audio_path)\n",
    "    video.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c.Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features_for_each_frame(audio_path, frame_rate=30,num_frames=None):\n",
    "    y, sr = librosa.load(audio_path)\n",
    "    \n",
    "    # frame_rate 1/1 = 30\n",
    "    # frame_rate 1/2 = 15\n",
    "\n",
    "    # Calculate the number of audio samples per video frame\n",
    "    samples_per_frame = sr / frame_rate\n",
    "\n",
    "    # Initialize an array to store MFCCs for each frame\n",
    "    mfccs_per_frame = []\n",
    "\n",
    "    # Iterate over each frame and extract corresponding MFCCs\n",
    "    for frame in range(int(len(y) / samples_per_frame)):\n",
    "        start_sample = int(frame * samples_per_frame)\n",
    "        end_sample = int((frame + 1) * samples_per_frame)\n",
    "\n",
    "        # Ensure the end sample does not exceed the audio length\n",
    "        end_sample = min(end_sample, len(y))\n",
    "\n",
    "        # Extract MFCCs for the current frame's audio segment\n",
    "        mfccs_current_frame = librosa.feature.mfcc(y=y[start_sample:end_sample], sr=sr, n_mfcc=13)\n",
    "        mfccs_processed = np.mean(mfccs_current_frame.T, axis=0)\n",
    "        mfccs_per_frame.append(mfccs_processed)\n",
    "\n",
    "    if(len(mfccs_per_frame)>num_frames):\n",
    "        return mfccs_per_frame[:num_frames]\n",
    "    return mfccs_per_frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Feature Extraction (example with visual features using a CNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Sequential\n",
    "import cv2\n",
    "import h5py\n",
    "\n",
    "# model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "# Load the weights from the downloaded file\n",
    "base_model = VGG16(weights=None, include_top=False)\n",
    "weights_path = 'vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5' # Replace with the actual path\n",
    "base_model.load_weights(weights_path)\n",
    "\n",
    "# Create a new Sequential model and add the VGG16 base model\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "\n",
    "def extract_visual_features(frames):\n",
    "    features = []\n",
    "    for frame in frames:\n",
    "        if frame is not None:\n",
    "            img = cv2.resize(frame, (224, 224))  # Resize frame to 224x224\n",
    "            img = img_to_array(img)              # Convert to array\n",
    "            img = np.expand_dims(img, axis=0)    # Add batch dimension\n",
    "            img = preprocess_input(img)          # Preprocess for VGG16\n",
    "            \n",
    "            feature = model.predict(img,use_multiprocessing=True,workers=4)\n",
    "            features.append(feature.flatten())\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect Audio/Annotation/Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotation To List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation2List(annotation_features):\n",
    "    # Make the string '1,1,1,3,2,2,4,4,1' to float list\n",
    "    annotation_float_array=[]\n",
    "    for annotation in annotation_features:\n",
    "        \n",
    "        if isinstance(annotation,str):\n",
    "            annotation = annotation.split(',')\n",
    "        if isinstance(annotation,list):\n",
    "            for anno in annotation:\n",
    "                annotation_float_array.append(float(anno))\n",
    "        else:\n",
    "            annotation_float_array.append(float(annotation))\n",
    "    return annotation_float_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title Tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfTitle(info_file,video_path):\n",
    "    info_df = pd.read_csv(info_file, sep='\\t')\n",
    "    info_video=info_df[info_df['video_id']==video_path.split('/')[-1].split('.')[0]]\n",
    "    title=info_video['title'].values[0]\n",
    "    # Preprocess titles and extract features (TF-IDF or word embeddings)\n",
    "    # For TF-IDF:\n",
    "    print('title:',title)\n",
    "\n",
    "    title_features = tokenizer.encode(title, add_special_tokens=False)\n",
    "    return np.array(title_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractData(video_path, anno_file, info_file,flag_to_extract):\n",
    "    \n",
    "    return_data=[]\n",
    "    # Extract frames from the video\n",
    "    if(flag_to_extract[0]):\n",
    "        frames = extract_frames(video_path)\n",
    "        return_data.append(['frames',frames])\n",
    "    else:\n",
    "        return_data.append(None)\n",
    "\n",
    "    # # Extract visual features\n",
    "    if(flag_to_extract[1]):\n",
    "        visual_features = extract_visual_features(frames) \n",
    "        return_data.append(['visual',visual_features])\n",
    "    else:\n",
    "        return_data.append(None)\n",
    "\n",
    "    # Extract audio\n",
    "    if(flag_to_extract[2]):\n",
    "        audio_output_path = 'datasets/extractedAudio/extracted_audio.wav'\n",
    "        extract_audio_from_video(video_path, audio_output_path) \n",
    "\n",
    "        # Extract audio features\n",
    "        audio_features = extract_audio_features_for_each_frame(audio_output_path,15,len(frames))\n",
    "        return_data.append(['audio',audio_features])\n",
    "    else:\n",
    "        return_data.append(None)\n",
    "\n",
    "    # Load titles from info file\n",
    "    if(flag_to_extract[3]):\n",
    "        title_features = tfTitle(info_file,video_path)\n",
    "        return_data.append(['title',title_features])\n",
    "    else:\n",
    "        return_data.append(None)\n",
    "\n",
    "    return return_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans and feature connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Detect objects and return padded encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectObjects(frames, yolo_model, classes, objects=None,video=None):\n",
    "    if objects is None:\n",
    "        print('Detecting objects in frames...')\n",
    "        yolo_model, classes = loadYOLOv5()\n",
    "        \n",
    "        objects = od.detect_objects_in_all_frames(frames, yolo_model, classes)\n",
    "        saveData('objects',objects,video)\n",
    "\n",
    "    # Here, taking the first detected object\n",
    "    objects = [frame_objects[0] if frame_objects else 'None' for frame_objects in objects]\n",
    "\n",
    "    # One-hot encoding of objects\n",
    "    encoded_objects=[]\n",
    "    for object in objects:\n",
    "        encoded_objects.append(tokenizer.encode(object, add_special_tokens=True))\n",
    "    encoded_objects = np.array(encoded_objects)\n",
    "\n",
    "    # Padding not required as one-hot encoding ensures consistent vector length\n",
    "    \n",
    "    return encoded_objects, objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### plot Data $Remove it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(combined_features_normalized, kmeans):\n",
    "\n",
    "    plt.figure(figsize=(20, 12))\n",
    "    plt.scatter(combined_features_normalized[:, 0], combined_features_normalized[:, 1], c=kmeans.labels_, cmap='viridis', marker='o', s=5)  # Reduced size\n",
    "\n",
    "    # Add labels to each point\n",
    "    for i, label in enumerate(kmeans.labels_):\n",
    "        plt.text(combined_features_normalized[i, 0], combined_features_normalized[i, 1], str(label), fontsize=8)  # Reduced fontsize\n",
    "\n",
    "    plt.title('KMeans Clustering')\n",
    "    plt.xlabel('PCA Feature 1')\n",
    "    plt.ylabel('PCA Feature 2')\n",
    "    plt.colorbar(label='Cluster Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent_in_rows(array):\n",
    "    most_frequent = []\n",
    "    for row in array:\n",
    "        counts = np.bincount(row)\n",
    "        most_frequent_number = np.argmax(counts)\n",
    "        most_frequent.append(most_frequent_number)\n",
    "    return most_frequent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save Data if ness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def saveData(name,feature,video):\n",
    "    if not os.path.exists(f'video_ext_data/{video}'):\n",
    "        os.makedirs(f'video_ext_data/{video}')\n",
    "    with open(f'video_ext_data/{video}/{name}.pkl', 'wb') as f:\n",
    "        pickle.dump(feature,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Data if ness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def getData(feature,video):\n",
    "    if os.path.exists(f'video_ext_data/{video}/{feature}.pkl'):\n",
    "        with open(f'video_ext_data/{video}/{feature}.pkl', 'rb') as f:\n",
    "            feature = pickle.load(f)\n",
    "        return feature\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiModalKMeans(video_path, anno_file, info_file,num_clusters=None,getDataFlag=False):\n",
    "    \"\"\"\n",
    "    Integrate visual, audio, and annotation features from a video,\n",
    "    and perform clustering on the combined features.\n",
    "\n",
    "    :param video_path: Path to the video file.\n",
    "    :param anno_file: Path to the annotation file.\n",
    "    :param info_file: Path to the info file.\n",
    "    :param num_clusters: Number of clusters to use in KMeans.\n",
    "    :return: Cluster labels for each data point.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract data from video\n",
    "    objects=None\n",
    "    \n",
    "    video=video_path.split('/')[-1].split('.')[0]\n",
    "    \n",
    "    # GetData\n",
    "    objects=getData('objects',video)\n",
    "    frames=getData('frames',video)\n",
    "    visual_features=getData('visual',video)\n",
    "    audio_features=getData('audio',video)\n",
    "    title_features=getData('title',video)\n",
    "    \n",
    "    flag_to_extract=[True,True,True,True]\n",
    "    \n",
    "    if(frames is not None):\n",
    "        flag_to_extract[0]=False\n",
    "    if(visual_features is not None):\n",
    "        flag_to_extract[1]=False\n",
    "    if(audio_features is not None):\n",
    "        flag_to_extract[2]=False\n",
    "    if(title_features is not None):\n",
    "        flag_to_extract[3]=False\n",
    "    \n",
    "    if not getDataFlag:\n",
    "        # Extract data from video and save it\n",
    "        data=extractData(video_path, anno_file, info_file,flag_to_extract)\n",
    "        # Save extracted Data\n",
    "        for d in data:\n",
    "            if d is not None:\n",
    "                if(d[0]=='objects'):\n",
    "                    objects=d[1]\n",
    "                elif(d[0]=='frames'):\n",
    "                    frames=d[1]\n",
    "                elif(d[0]=='visual'):\n",
    "                    visual_features=d[1]\n",
    "                elif(d[0]=='audio'):\n",
    "                    audio_features=d[1]\n",
    "                elif(d[0]=='title'):\n",
    "                    title_features=d[1]\n",
    "                \n",
    "                saveData(d[0],d[1],video)\n",
    "            \n",
    "\n",
    "    if(objects is None):\n",
    "        padded_encoded_objects,objects = detectObjects(frames,len(visual_features),len(audio_features),video=video)\n",
    "    else:\n",
    "        padded_encoded_objects,objects = detectObjects(frames,len(visual_features),len(audio_features),objects)\n",
    "\n",
    "    combined_features = []\n",
    "    for i, frame in enumerate(frames):\n",
    "        # annotation_float_array = annotation2List(annotation_features[i])\n",
    "        combined_feature = np.concatenate([\n",
    "            # np.array(frame.flatten(), dtype=float),\n",
    "            np.array(visual_features[i], dtype=float),\n",
    "            np.array(audio_features[i], dtype=float),\n",
    "            # padded_encoded_objects[i],\n",
    "            # np.array(title_features, dtype=float).reshape(-1),\n",
    "        ])\n",
    "        combined_features.append(combined_feature)\n",
    "\n",
    "    # Convert to 2D NumPy array and normalize    \n",
    "    combined_features_normalized = StandardScaler().fit_transform(np.array(combined_features))\n",
    "    \n",
    "    # num_clusters= ceil(sqrt(sqrt(len(combined_features_normalized) * 2)))\n",
    "    \n",
    "    # print(\"Number of clusters:\", num_clusters)\n",
    "    \n",
    "    # Perform clustering\n",
    "    print(\"Performing clustering...\")\n",
    "    \n",
    "    # try:\n",
    "    #     parameters = {\n",
    "    #         'K':10,            # Max clusters: Higher value for more clusters\n",
    "    #         'I': 100,           # Max iterations: Lower this for quicker runs during initial testing\n",
    "    #         'P': 200,            # Min population: Allows for smaller clusters to merge\n",
    "    #         'THETA_M': 1,     # Min distance between clusters: Smaller value to prevent large cluster merges\n",
    "    #         'THETA_S': 0.8,    # Min standard deviation for splitting: Adjust based on the spread of your data\n",
    "    #         'THETA_C': 0.8,    # Min distance between centroids for merging: A moderate value for centroid closeness\n",
    "    #     }\n",
    "    #     labels = isodata.isodata_classification(combined_features_normalized, parameters)\n",
    "    #     labels=most_frequent_in_rows(labels)\n",
    "\n",
    "    #     # print(\"First few labels:\", labels[:5])\n",
    "\n",
    "    #     # print(img_class_flat)\n",
    "    #     # print(img_class_flat.shape)\n",
    "    #     # print(np.bincount(img_class_flat[:,0]))\n",
    "    #     # labels = np.array([np.argmax(np.bincount(img_class_flat[:,i])) for i in range(img_class_flat.shape[1])])\n",
    "                        \n",
    "    #     return [frames, labels, padded_encoded_objects,title_features]\n",
    "    # except Exception as e:\n",
    "    #     print('Error during ISODATA clustering:', e)\n",
    "\n",
    "\n",
    "    try:\n",
    "        if num_clusters:\n",
    "            kmeans = KMeans(n_clusters=num_clusters) \n",
    "            kmeans.fit(combined_features_normalized)\n",
    "            # plotData(combined_features_normalized,kmeans) #Delete later\n",
    "            return [frames, kmeans.labels_,padded_encoded_objects,title_features]\n",
    "    except:\n",
    "        print('Error None number of clusters')\n",
    "    # return the frames and the labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video_from_frames(frames, output_path, frame_rate=30):\n",
    "    if not frames:\n",
    "        print(\"No frames to create a video.\")\n",
    "        return None\n",
    "    # Determine the width and height from the first frame\n",
    "    height, width, layers = frames[0].shape\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, frame_rate, (width, height))\n",
    "    \n",
    "    # Write each frame to the video\n",
    "    for frame in frames:\n",
    "        out.write(frame)\n",
    "\n",
    "    # Release the VideoWriter object\n",
    "    out.release()\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load .Mat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_titles(encoded_titles, hdf5_file):\n",
    "    decoded_titles = []\n",
    "    for ref_array in encoded_titles:\n",
    "        # Handle the case where each ref_array might contain multiple references\n",
    "        for ref in ref_array:\n",
    "            # Dereference each HDF5 object reference to get the actual data\n",
    "            title_data = hdf5_file[ref]\n",
    "            # Decode the title\n",
    "            decoded_title = ''.join(chr(char[0]) for char in title_data)\n",
    "            decoded_titles.append(decoded_title)\n",
    "    return decoded_titles\n",
    "\n",
    "\n",
    "def load_mat_file(file_path,videoID):\n",
    "    \"\"\"\n",
    "    Load a .mat file and return its contents.\n",
    "\n",
    "    :param file_path: Path to the .mat file.\n",
    "    :return: Contents of the .mat file.\n",
    "    \"\"\"\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        user_anno_refs=file['tvsum50']['user_anno'][:] # type: ignore\n",
    "        video_refs=file['tvsum50']['video'][:] # type: ignore\n",
    "\n",
    "        decoded_videos = decode_titles(video_refs,file)\n",
    "    \n",
    "        annotations = []        \n",
    "        # Get the index from decoded video list to find the annotation for the video\n",
    "        index = [i for i, x in enumerate(decoded_videos) if x.lower() in videoID.lower()][0]\n",
    "        \n",
    "        # Iterate over each reference\n",
    "        for ref in user_anno_refs:\n",
    "            # Dereference each HDF5 object reference\n",
    "            ref_data = file[ref[0]]\n",
    "\n",
    "            # Convert to NumPy array and add to the annotations list\n",
    "            annotations.append(np.array(ref_data))\n",
    "            \n",
    "        return annotations[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f1score with ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_frame_selection(ground_truth, summary_indices):\n",
    "    \"\"\"\n",
    "    Evaluate the selected frames by comparing them with the ground truth.\n",
    "    \n",
    "    Args:\n",
    "    ground_truth: Ground truth annotations.\n",
    "    summary_indices: Indices of the selected frames.\n",
    "    \n",
    "    Returns:\n",
    "    Average importance score, max importance score, and proportion of frames with high importance score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Evaluate the selected frames\n",
    "    selected_importance_scores = ground_truth[summary_indices]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    if selected_importance_scores.size == 0:\n",
    "        average_importance = 0\n",
    "        max_importance = 0\n",
    "        proportion_high_importance = 0\n",
    "    else:\n",
    "        average_importance = np.mean(selected_importance_scores)  # Average importance score\n",
    "        max_importance = np.max(selected_importance_scores)  # Max importance score\n",
    "        # Calculate the proportion of frames with high importance score\n",
    "        proportion_high_importance = np.mean(selected_importance_scores >= np.floor(max_importance))\n",
    "        \n",
    "    return average_importance, max_importance,proportion_high_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evaluation(ground_truth_path,summary_indices,videoID):\n",
    "    \n",
    "    # Get the ground_truth\n",
    "    ground_truth = np.array(load_mat_file(ground_truth_path, videoID))\n",
    "    \n",
    "    # Find the mean of all annotators\n",
    "    gold_standard = np.mean(ground_truth, axis=0)\n",
    "    \n",
    "    # based on annotators find the avg_importance, max_importance, prop_high_importance\n",
    "    avg_importance, max_importance, prop_high_importance = evaluate_frame_selection(gold_standard, summary_indices)\n",
    "\n",
    "    # Thresholding based on max_importance\n",
    "    threshold=floor(np.mean(avg_importance))\n",
    "    \n",
    "    \n",
    "    # Binary conversion\n",
    "    binary_ground_truth = np.where(gold_standard >= threshold, 1, 0) \n",
    "    \n",
    "    # Selected frames binary conversion\n",
    "    selected_frames_binary = np.zeros_like(binary_ground_truth)\n",
    "    selected_frames_binary[summary_indices] = 1\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = precision_score(binary_ground_truth, selected_frames_binary)\n",
    "    recall = recall_score(binary_ground_truth, selected_frames_binary)  \n",
    "    f1 = f1_score(binary_ground_truth, selected_frames_binary, average='macro')\n",
    "    \n",
    "    # return metrics\n",
    "    return threshold,precision,recall,f1,avg_importance,max_importance,prop_high_importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KnapSack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knapsack_for_video_summary(values, weights, capacity, scale_factor=30):\n",
    "    \"\"\"\n",
    "    Apply the 0/1 Knapsack algorithm to select video segments for summarization.\n",
    "\n",
    "    :param values: List of importance scores for each segment.\n",
    "    :param weights: List of durations for each segment in seconds.\n",
    "    :param capacity: Maximum total duration for the summary in seconds.\n",
    "    :param scale_factor: Factor to scale weights to integers.\n",
    "    :return: Indices of the segments to include in the summary.\n",
    "    \"\"\"\n",
    "    # Scale weights and capacity\n",
    "    weights = [int(w * scale_factor) for w in weights]\n",
    "    capacity = int(capacity * scale_factor)\n",
    "\n",
    "    n = len(values)\n",
    "    K = [[0 for _ in range(capacity + 1)] for _ in range(n + 1)]\n",
    "\n",
    "    # Build table K[][] in a bottom-up manner\n",
    "    for i in range(n + 1):\n",
    "        for w in range(capacity + 1):\n",
    "            if i == 0 or w == 0:\n",
    "                K[i][w] = 0\n",
    "            elif weights[i-1] <= w:\n",
    "                K[i][w] = max(values[i-1] + K[i-1][w-weights[i-1]], K[i-1][w])\n",
    "            else:\n",
    "                K[i][w] = K[i-1][w]\n",
    "\n",
    "    # Find the selected segments\n",
    "    res = K[n][capacity]\n",
    "    w = capacity\n",
    "    selected_indices = []\n",
    "\n",
    "    for i in range(n, 0, -1):\n",
    "        if res <= 0:\n",
    "            break\n",
    "        if res == K[i-1][w]:\n",
    "            continue\n",
    "        else:\n",
    "            selected_indices.append(i-1)\n",
    "            res = res - values[i-1]\n",
    "            w = w - weights[i-1]\n",
    "\n",
    "    selected_indices.reverse()\n",
    "    return selected_indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### importance score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_importance_ManDistFrame(title,objects):\n",
    "    # Calculate the importance score for each frame using Manhattan distance\n",
    "    importance_scores = []\n",
    "    \n",
    "    for obj in objects:\n",
    "        # Ensure both vectors have the same length\n",
    "        obj_array = np.array(obj)\n",
    "        title_array = np.array(title)\n",
    "        # If lengths are different, pad the shorter array with zeros\n",
    "        length_difference = len(title_array) - obj_array.size\n",
    "        if length_difference > 0:\n",
    "            obj_array = np.pad(obj_array, (0, length_difference), 'constant')\n",
    "        elif length_difference < 0:\n",
    "            title_array = np.pad(title_array, (0, -length_difference), 'constant')\n",
    "\n",
    "        # Calculate Manhattan distance\n",
    "        distance = np.sum(np.abs(obj_array - title_array))\n",
    "        importance_scores.append(distance)\n",
    "\n",
    "    importance_scores = np.array(importance_scores)\n",
    "    importance_scores = importance_scores / np.max(importance_scores)  # Normalize\n",
    "\n",
    "    # Invert the scores because lower Manhattan distance indicates higher similarity\n",
    "    importance_scores = 1 - importance_scores\n",
    "    \n",
    "    for i in range(len(importance_scores)):\n",
    "        if importance_scores[i]==0:\n",
    "            importance_scores[i]=0.0001\n",
    "\n",
    "    return importance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_importance_scores_for_cluster(cluster_frames, detected_objects_per_frame):\n",
    "\n",
    "    # Initialize importance scores for the cluster\n",
    "    importance_scores = []\n",
    "\n",
    "    # Calculate object-based importance for each frame in the cluster\n",
    "    for frame_index in cluster_frames:\n",
    "        # Calculate object importance\n",
    "        objects = detected_objects_per_frame[frame_index]\n",
    "        object_importance = len(objects) if objects else 0\n",
    "\n",
    "        # Add object importance to the list\n",
    "        importance_scores.append(object_importance)\n",
    "\n",
    "    # Normalize the importance scores\n",
    "    max_importance = max(importance_scores, default=1)\n",
    "    if max_importance == 0:\n",
    "        max_importance = 1  # To prevent division by zero\n",
    "\n",
    "    normalized_importance_scores = [score / max_importance for score in importance_scores]\n",
    "\n",
    "    return normalized_importance_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map labels with frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_frames_to_labels_with_indices(frames, labels):\n",
    "    label_frame_dict = {}\n",
    "    for label, (frame_index, frame) in zip(labels, enumerate(frames)):\n",
    "        # Convert label from numpy array to scalar if necessary\n",
    "        label_scalar = label.item() if isinstance(label, np.ndarray) else label\n",
    "        if label_scalar not in label_frame_dict:\n",
    "            label_frame_dict[label_scalar] = []\n",
    "        label_frame_dict[label_scalar].append((frame_index, frame))\n",
    "    return label_frame_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Delete pkl Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deletePKLfiles(video):\n",
    "    dirpkl='video_ext_data/'+video+'/'\n",
    "    if(os.path.exists(dirpkl+'frames.pkl')):\n",
    "        os.remove(dirpkl+\"frames.pkl\")\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to create summary video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_path='datasets/ydata-tvsum50-v1_1/data/ydata-tvsum50-anno.tsv'\n",
    "info_path='datasets/ydata-tvsum50-v1_1/data/ydata-tvsum50-info.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path='datasets/ydata-tvsum50-v1_1/video/'\n",
    "summary_video_path='datasets/summary_videos/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_path='datasets/ydata-tvsum50-v1_1/ground_truth/ydata-tvsum50.mat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get the list of the videos in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_list = [video for video in os.listdir(video_path) if video.endswith('.mp4')]  # List comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Yolo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def loadYOLOv5():\n",
    "    # Load the model\n",
    "    yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True,)\n",
    "\n",
    "    with open(\"yolo/coco.names\", \"r\") as f:\n",
    "        classes = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "    return yolo_model,classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_rate = 30  # or whatever your frame rate is\n",
    "frame_duration = 1 / frame_rate  # Duration of each frame in seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map the mat file with h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last change point and total frames for each video in the .h5 file\n",
    "def get_video_data_from_h5(file_path):\n",
    "    video_data_h5 = []\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        for video_id in file.keys():\n",
    "            last_change_point = file[str(video_id)]['change_points'][-1]\n",
    "            total_frames = last_change_point[1]\n",
    "            video_data_h5.append([video_id, total_frames])\n",
    "    return video_data_h5\n",
    "\n",
    "# Get frame numbers from the .mat file\n",
    "def get_frame_numbers(encoded_frames, hdf5_file):\n",
    "    frame_numbers = []\n",
    "    for ref_array in encoded_frames:\n",
    "        for ref in ref_array:\n",
    "            frame_data = hdf5_file[ref]\n",
    "            frame_numbers.extend([int(char[0]) for char in frame_data])\n",
    "    return frame_numbers\n",
    "\n",
    "# Extract data from .mat file\n",
    "def get_video_data_from_mat(file_path):\n",
    "    video_data_mat = []\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        encoded_videos = f['tvsum50']['video'][:]\n",
    "        encoded_frame_counts = f['tvsum50']['nframes'][:]\n",
    "        decoded_videos = decode_titles(encoded_videos, f)\n",
    "        decoded_frame_counts = get_frame_numbers(encoded_frame_counts, f)\n",
    "        for i, video_id in enumerate(decoded_videos):\n",
    "            video_data_mat.append([video_id, decoded_frame_counts[i]])\n",
    "    return video_data_mat\n",
    "\n",
    "def getChangingPoints(file_path,video_id):\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        return file[video_id]['change_points'][:]\n",
    "    \n",
    "# Comparing and mapping the data\n",
    "h5_file_path = 'datasets/ydata-tvsum50-v1_1/eccv16_dataset_tvsum_google_pool5.h5'\n",
    "mat_file_path = 'datasets/ydata-tvsum50-v1_1/ground_truth/ydata-tvsum50.mat'\n",
    "\n",
    "video_data_h5 = get_video_data_from_h5(h5_file_path)\n",
    "video_data_mat = get_video_data_from_mat(mat_file_path)\n",
    "\n",
    "video_id_map = {}\n",
    "for video_mat in video_data_mat:\n",
    "    for video_h5 in video_data_h5:\n",
    "        if video_mat[1] == video_h5[1] + 1:\n",
    "            video_id_map[video_mat[0]] = video_h5[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for videoSummarizion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_scores_to_original_frames(sampled_scores, frame_rate):\n",
    "    # Create an empty list to hold the mapped scores\n",
    "    original_scores = []\n",
    "\n",
    "    # Iterate over the sampled scores\n",
    "    for score in sampled_scores:\n",
    "        # Replicate each score frame_rate times\n",
    "        original_scores.extend([score] * frame_rate)\n",
    "\n",
    "    return original_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "def videoSumm(annotation_path=None, info_path=None, video_path=None, summary_video_path=None,video_list=None):\n",
    "# video in video_list: \n",
    "        video='xxdtq8mxegs.mp4'\n",
    "\n",
    "        # if(os.path.exists(f'{summary_video_path}{video}')):\n",
    "            # continue\n",
    "        max_model=None\n",
    "        getDataFlag=False\n",
    "        bet_num_clusters=None\n",
    "        num_clusters=[3,5,7,9,11,13,15]\n",
    "        getDataFlag=False\n",
    "        \n",
    "        for num in num_clusters:\n",
    "            frames,labels,objects,title_features = MultiModalKMeans(video_path+video, annotation_path, info_path,num_clusters=num,getDataFlag=getDataFlag)\n",
    "            \n",
    "            # For the next iteration we don't need to extract the data again\n",
    "            getDataFlag=True\n",
    "            \n",
    "            \"\"\" USE THIS IN CASE OF CREATING VIDEO FOR EACH CLUSTER\"\"\"\n",
    "            cluster_dict = map_frames_to_labels_with_indices(frames, labels)\n",
    "            \n",
    "            # keep track of scores\n",
    "            scores = []\n",
    "            model_videos=[]\n",
    "            for index, cluster in cluster_dict.items():\n",
    "                capacity = len(frames)*0.15\n",
    "                print(\"Calculate Cluster:\", index)\n",
    "                # Extract just the frame indices for importance score calculation\n",
    "                cluster_frame_indices = [frame_index for frame_index, _ in cluster]\n",
    "\n",
    "                # Calculate importance scores for this cluster\n",
    "                importance = calculate_importance_scores_for_cluster(cluster_frame_indices, objects)\n",
    "\n",
    "                # Create a list of durations, one for each frame in the cluster\n",
    "                durations = [frame_duration] * len(importance)\n",
    "\n",
    "                # Apply the knapsack algorithm\n",
    "                summary_indices = knapsack_for_video_summary(importance, durations, capacity)\n",
    "\n",
    "                # Select Representative Frames (extracting frame part from each tuple)\n",
    "                summary_frames = [cluster[i][1] for i in summary_indices]  # [1] extracts the frame from (frame_index, frame)\n",
    "                \n",
    "                # Extract original indices for evaluation\n",
    "                # i want to extract the original indices from the cluster cause the summary_indices are based on the all frames\n",
    "                # so i need to extract the original indices from each cluster\n",
    "                original_indices_for_eval = [cluster[i][0] for i in summary_indices]  # [0] extracts the original frame index\n",
    "\n",
    "                # Evaluate\n",
    "                evaluated_metrics = Evaluation(ground_truth_path, original_indices_for_eval, video.split('.')[0])\n",
    "                \n",
    "                # Append index to metrics\n",
    "                scores.append((str(index),) + evaluated_metrics)\n",
    "                \n",
    "                # Get the model_video based on the index\n",
    "                model_videos.append([str(index),summary_frames,f\"{summary_video_path}{index}-{video}\"])\n",
    "                \n",
    "            # Sort the table of scores based on F1 score\n",
    "            scores.sort(key=lambda x: x[4], reverse=True)\n",
    "            \n",
    "            if((max_model is None )or (max_model[4] < scores[0][4])):\n",
    "                max_model=scores[0]\n",
    "                bet_num_clusters=num\n",
    "                \n",
    "            # print(tabulate(scores, headers=['videoID','Threshold','Precision', 'Recall', 'F1 Score', 'Avg_importance', 'Max_importance', 'Prop_high_importance'], tablefmt='fancy_grid'))\n",
    "\n",
    "        print(\"VIDEO\",video)\n",
    "        print(f\"Best model clusters: {bet_num_clusters}\",max_model)\n",
    "        index, summary_frames, output_path = next((model_video for model_video in model_videos if model_video[0] == max_model[0]), (None, None, None))        \n",
    "        \n",
    "        print(f\"Best model: {index}\",output_path)\n",
    "        \n",
    "        video_name=video.split('.')[0]\n",
    "        \n",
    "        if(not os.path.exists(f'video_ext_data/{video_name}')):\n",
    "            os.makedirs(f'video_ext_data/{video_name}')\n",
    "        with open(f'video_ext_data/{video_name}/model.pkl', 'wb') as f:\n",
    "            pickle.dump(model_videos,f)\n",
    "        with open(f'video_ext_data/{video_name}/max_model.pkl', 'wb') as f:\n",
    "            pickle.dump(max_model,f)\n",
    "        \n",
    "        # fix output_path\n",
    "        output_path=output_path.replace(f'{index}-','')\n",
    "        # Create Summary Video\n",
    "        create_video_from_frames(summary_frames, output_path, 30)\n",
    "        \n",
    "        del model_videos\n",
    "        del scores\n",
    "        del summary_frames\n",
    "        del importance\n",
    "        del cluster_frame_indices\n",
    "        \n",
    "        # Extract data from video the next video\n",
    "        getDataFlag=False\n",
    "        deletePKLfiles(video_name)\n",
    "    # return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "def videoSumm2(annotation_path=None, info_path=None, video_path=None, summary_video_path=None,video_list=None):\n",
    "# video in video_list: \n",
    "        video='xxdtq8mxegs.mp4'\n",
    "\n",
    "        # if(os.path.exists(f'{summary_video_path}{video}')):\n",
    "            # continue\n",
    "        max_model=None\n",
    "        getDataFlag=False\n",
    "        bet_num_clusters=None\n",
    "        num_clusters=[3,5,7,9,11,13,15]\n",
    "        getDataFlag=False\n",
    "        \n",
    "        original_frames=extract_frames(video_path+video, frame_rate=1)\n",
    "        \n",
    "        for num in num_clusters:\n",
    "            frames,labels,objects,title_features = MultiModalKMeans(video_path+video, annotation_path, info_path,num_clusters=num,getDataFlag=getDataFlag)\n",
    "            \n",
    "            # For the next iteration we don't need to extract the data again\n",
    "            getDataFlag=True\n",
    "            \n",
    "            # keep track of scores\n",
    "            scores = []\n",
    "            model_videos=[]\n",
    "            \n",
    "            # Calculate importance scores for this cluster\n",
    "            importance = calculate_importance_ManDistFrame(title_features, objects)\n",
    "            \n",
    "            importance=map_scores_to_original_frames(importance, 2)\n",
    "                \n",
    "            capacity = len(frames)*0.15\n",
    "            print(\"Calculate Cluster:\", num)\n",
    "\n",
    "            # Create a list of durations, one for each frame in the cluster\n",
    "            durations = [frame_duration] * len(importance)\n",
    "\n",
    "            # Apply the knapsack algorithm\n",
    "            summary_indices = knapsack_for_video_summary(importance, durations, capacity)\n",
    "            \n",
    "            summary_frames = [original_frames[i] for i in summary_indices]\n",
    "\n",
    "            # Evaluate\n",
    "            evaluated_metrics = Evaluation(ground_truth_path, summary_indices, video.split('.')[0])\n",
    "            \n",
    "            # Append index to metrics\n",
    "            scores.append(evaluated_metrics)\n",
    "            \n",
    "            # Get the model_video based on the index\n",
    "            model_videos.append([summary_frames,f\"{summary_video_path}-{video}\"])\n",
    "            \n",
    "        # Sort the table of scores based on F1 score\n",
    "        scores.sort(key=lambda x: x[4], reverse=True)\n",
    "        \n",
    "        if((max_model is None )or (max_model[4] < scores[0][4])):\n",
    "            max_model=scores[0]\n",
    "            bet_num_clusters=num\n",
    "            \n",
    "        print(tabulate(scores, headers=['videoID','Threshold','Precision', 'Recall', 'F1 Score', 'Avg_importance', 'Max_importance', 'Prop_high_importance'], tablefmt='fancy_grid'))\n",
    "\n",
    "        print(\"VIDEO\",video)\n",
    "        print(f\"Best model clusters: {bet_num_clusters}\",max_model)\n",
    "        summary_frames, output_path = next((model_video for model_video in model_videos if model_video[0] == max_model[0]), ( None, None))        \n",
    "        \n",
    "        print(f\"Best model:{output_path}\")\n",
    "        \n",
    "        video_name=video.split('.')[0]\n",
    "        \n",
    "        if(not os.path.exists(f'video_ext_data/{video_name}')):\n",
    "            os.makedirs(f'video_ext_data/{video_name}')\n",
    "        with open(f'video_ext_data/{video_name}/model.pkl', 'wb') as f:\n",
    "            pickle.dump(model_videos,f)\n",
    "        with open(f'video_ext_data/{video_name}/max_model.pkl', 'wb') as f:\n",
    "            pickle.dump(max_model,f)\n",
    "        \n",
    "        # Create Summary Video\n",
    "        create_video_from_frames(summary_frames, output_path, 30)\n",
    "        \n",
    "        del model_videos\n",
    "        del scores\n",
    "        del summary_frames\n",
    "        del importance\n",
    "        del cluster_frame_indices\n",
    "        \n",
    "        # Extract data from video the next video\n",
    "        getDataFlag=False\n",
    "        deletePKLfiles(video_name)\n",
    "    # return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TREXE AYTO EDW PANW GIATI TO ALLAJA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for dynamic annotation and video summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    " - Evaluation based on weights and change points\n",
    " - shows correct the evaluation for each clustering number\n",
    " - use isodata for cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing clustering...\n",
      "2162\n",
      "Calculate Cluster: 3\n",
      "Performing clustering...\n",
      "2162\n",
      "Calculate Cluster: 5\n",
      "Performing clustering...\n",
      "2162\n",
      "Calculate Cluster: 7\n",
      "Performing clustering...\n",
      "2162\n",
      "Calculate Cluster: 9\n",
      "Performing clustering...\n",
      "2162\n",
      "Calculate Cluster: 11\n",
      "Performing clustering...\n",
      "2162\n",
      "Calculate Cluster: 13\n",
      "Performing clustering...\n",
      "2162\n",
      "Calculate Cluster: 15\n",
      "╒═══════════╤═════════════╤═════════════╤══════════╤════════════╤══════════════════╤══════════════════╕\n",
      "│   videoID │   Threshold │   Precision │   Recall │   F1 Score │   Avg_importance │   Max_importance │\n",
      "╞═══════════╪═════════════╪═════════════╪══════════╪════════════╪══════════════════╪══════════════════╡\n",
      "│         1 │           1 │           1 │        1 │    1.97922 │              3.9 │        0.0832562 │\n",
      "╘═══════════╧═════════════╧═════════════╧══════════╧════════════╧══════════════════╧══════════════════╛\n",
      "VIDEO xxdtq8mxegs.mp4\n",
      "Best model clusters: 15 (1, 1.0, 1.0, 1.0, 1.9792206290471785, 3.9, 0.08325624421831637)\n",
      "Best model:None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[513], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvideoSumm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotation_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummary_video_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[512], line 75\u001b[0m, in \u001b[0;36mvideoSumm2\u001b[0;34m(annotation_path, info_path, video_path, summary_video_path, video_list)\u001b[0m\n\u001b[1;32m     72\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(max_model,f)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# fix output_path\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m output_path\u001b[38;5;241m=\u001b[39m\u001b[43moutput_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Create Summary Video\u001b[39;00m\n\u001b[1;32m     77\u001b[0m create_video_from_frames(summary_frames, output_path, \u001b[38;5;241m30\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "videoSumm2(annotation_path, info_path, video_path, summary_video_path, video_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD STUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/macbookpro2017/Desktop/UnsupervisedVideoSummarization/pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[307], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m fol\u001b[38;5;241m=\u001b[39m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/macbookpro2017/Desktop/UnsupervisedVideoSummarization/pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m data\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fol:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/macbookpro2017/Desktop/UnsupervisedVideoSummarization/pkl'"
     ]
    }
   ],
   "source": [
    "\n",
    "fol=os.listdir('/Users/macbookpro2017/Desktop/UnsupervisedVideoSummarization/pkl')\n",
    "\n",
    "data=[]\n",
    "for f in fol:\n",
    "    with open('video_ext_data/'+f+'/max_model.pkl','rb') as file:\n",
    "        df=pickle.load(file)\n",
    "        data.append((f,df[0],df[1],df[2],df[3],df[4],df[5],df[6]))\n",
    "\n",
    "# sort\n",
    "data.sort(key=lambda x: x[5], reverse=True)\n",
    "from tabulate import tabulate\n",
    "print(tabulate(data, headers=['video','Cluster','Threshold','Precision', 'Recall', 'F1 Score', 'Avg_importance', 'Max_importance', 'Prop_high_importance']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_anno='datasets/ydata-tvsum50-v1_1/data/ydata-tvsum50-anno.tsv'\n",
    "with open(dir_anno,'r') as file:\n",
    "    annotations=[]\n",
    "    df=pd.read_csv(file,sep='\\t')\n",
    "    for d in df.values:\n",
    "        if(d[0]=='xxdtq8mxegs'):\n",
    "            annotations.append(d[2])\n",
    "            \n",
    "    print(len(annotations[0]))\n",
    "    # print(df[df[0][0]=='0a0XQ4oA1-g'])\n",
    "    # print(df[0][1].values[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK\n",
    "* googleNet dataloader gia feature extraction\n",
    "* spase ta dedomena ena ena\n",
    "* video summary\n",
    "    *  vriskw ena tropo na kanw output simantikotita toy frame \n",
    "    * afoy ginei ayto kanw ton alg knapsack afoy prepei nanai 15% toy arxikoy video DONE\n",
    "    *  meta to evaluation pairnw analoga ta shots\n",
    "    *  san weight posa frame exei to plano\n",
    "    *  epiligei plano 1-3-5 px kai meta pas sto groundtruth kai tote fscore\n",
    "    * analoga to plano m.o.=avg.scoring kai weight=frames in plano\n",
    "### Learning\n",
    "* Cluster alg macmini - isodata xwris num_clustering\n",
    "* object gia titlo sygkrisi -> evaluation -> dianisma titloy,  dianisma leji gata \n",
    "    * an pareis titlo kai objects me idio tokenazier idio lejilogio me diansimatiko xoro. DONE\n",
    "    * meta me eyklidia apostasi, poies lejeis einai pio konta stin anaparastasi toy titloy kai met ranking\n",
    "    * object me titlo, meta lew oti opoio frame exei objects me ipsilo ranking kai norm gia 0-1\n",
    "    * rank - > pio kontino object titlo pairnei rank 1. endiamesa kanoyn scale analoga ta max kai min\n",
    "    * poia frame exoyn ayto to object ai pairnoyn rank\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
