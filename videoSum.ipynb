{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from sklearn.cluster import KMeans\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from math import floor,sqrt\n",
    "from moviepy.editor import VideoFileClip\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import h5py\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotations loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations\n",
    "def load_annotations(anno_file, info_file):\n",
    "    # Read the annotation and info files\n",
    "    annotations = pd.read_csv(anno_file, sep='\\t', header=None)\n",
    "    info = pd.read_csv(info_file, sep='\\t', header=None)\n",
    "    \n",
    "    # Rename columns for better understanding\n",
    "    annotations.columns = ['video_id', 'category', 'importance_score']\n",
    "    info.columns = ['category_code', 'video_id', 'title', 'url', 'length']\n",
    "    \n",
    "    return annotations, info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Feature Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Video Processing and Frame Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, frame_rate=25):\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    count = 0\n",
    "    success = True\n",
    "    frames = []\n",
    "    \n",
    "    while success:\n",
    "        success, image = video.read()\n",
    "        if count % frame_rate == 0 and success:\n",
    "            frames.append(image)\n",
    "        count += 1\n",
    "\n",
    "    video.release()\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Audio Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_audio_from_video(video_path, output_audio_path):\n",
    "    video = VideoFileClip(video_path)\n",
    "    audio = video.audio\n",
    "    audio.write_audiofile(output_audio_path)\n",
    "    video.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c.Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features(audio_path):\n",
    "    \"\"\"\n",
    "    Extract audio features from an audio file, specifically MFCCs.\n",
    "\n",
    "    :param audio_path: Path to the audio file.\n",
    "    :return: Array of MFCCs.\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(audio_path)\n",
    "\n",
    "    # Extract MFCCs from the audio\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)  # You can change n_mfcc based on your needs\n",
    "\n",
    "    # To capture the variation over time, you might compute statistics across MFCCs over time\n",
    "    # Here, we compute the mean of the MFCCs across time\n",
    "    mfccs_processed = np.mean(mfccs.T,axis=0)\n",
    "\n",
    "    return mfccs_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Feature Extraction (example with visual features using a CNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "def extract_visual_features(frames):\n",
    "    features = []\n",
    "    for frame in frames:\n",
    "        if frame is not None:\n",
    "            img = cv2.resize(frame, (224, 224))  # Resize frame to 224x224\n",
    "            img = img_to_array(img)        # Convert to array\n",
    "            img = np.expand_dims(img, axis=0)    # Add batch dimension\n",
    "            img = preprocess_input(img)          # Preprocess for VGG16\n",
    "            \n",
    "            feature = model.predict(img,use_multiprocessing=True,workers=4)\n",
    "            features.append(feature.flatten())\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect Audio/Annotation/Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding Features to fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(visual_features, audio_features, annotation_features):\n",
    "    # inputs are at least 1D arrays\n",
    "    visual_features = [np.array(v, ndmin=1, dtype=float) for v in visual_features]\n",
    "    audio_features = np.array(audio_features, ndmin=1, dtype=float)\n",
    "    annotation_features = [np.array(a, ndmin=1, dtype=float) for a in annotation_features]\n",
    "\n",
    "    # Find the maximum length of the features\n",
    "    max_length = max(\n",
    "        max(v.size for v in visual_features),\n",
    "        audio_features.size,\n",
    "        max(a.size for a in annotation_features)\n",
    "    )\n",
    "\n",
    "    # Pad features to match the maximum length\n",
    "    visual_padded = [np.pad(v, (0, max_length - v.size), 'constant') for v in visual_features]\n",
    "    audio_padded = np.pad(audio_features, (0, max_length - audio_features.size), 'constant')\n",
    "    annotation_padded = [np.pad(a, (0, max_length - a.size), 'constant') for a in annotation_features]\n",
    "\n",
    "    return visual_padded, audio_padded, annotation_padded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotation To List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation2List(annotation_features):\n",
    "    # Make the string '1,1,1,3,2,2,4,4,1' to float list\n",
    "    annotation_float_array=[]\n",
    "    for annotation in annotation_features:\n",
    "        \n",
    "        if isinstance(annotation,str):\n",
    "            annotation = annotation.split(',')\n",
    "        if isinstance(annotation,list):\n",
    "            for anno in annotation:\n",
    "                annotation_float_array.append(float(anno))\n",
    "        else:\n",
    "            annotation_float_array.append(float(annotation))\n",
    "    return annotation_float_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add weights to ignore padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_feature_weighting(combined_features, original_feature_length, original_feature_weight=1, padding_weight=0.1):\n",
    "    \"\"\"\n",
    "    Apply different weights to the original and padded parts of the feature vectors.\n",
    "\n",
    "    :param combined_features: List of combined feature vectors (including padding).\n",
    "    :param original_feature_length: The length of the original (non-padded) part of the feature vectors.\n",
    "    :param original_feature_weight: Weight to be applied to the original features.\n",
    "    :param padding_weight: Weight to be applied to the padded features.\n",
    "    :return: List of weighted feature vectors.\n",
    "    \"\"\"\n",
    "    weighted_combined_features = []\n",
    "\n",
    "    for feature in combined_features:\n",
    "        # Create a weight vector: higher weight for original features, lower for padding\n",
    "        weights = [original_feature_weight] * original_feature_length\n",
    "        weights += [padding_weight] * (len(feature) - original_feature_length)\n",
    "        \n",
    "        # Apply weights to the feature vector\n",
    "        weighted_feature = np.array(feature) * np.array(weights)\n",
    "        weighted_combined_features.append(weighted_feature)\n",
    "\n",
    "    return weighted_combined_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractData(video_path, anno_file, info_file):\n",
    "    # Extract frames from the video\n",
    "    frames = extract_frames(video_path) \n",
    "\n",
    "    # # Extract visual features\n",
    "    visual_features = extract_visual_features(frames) \n",
    "\n",
    "    # Extract audio\n",
    "    audio_output_path = 'datasets/extractedAudio/extracted_audio.wav'\n",
    "    extract_audio_from_video(video_path, audio_output_path) \n",
    "\n",
    "    # Extract audio features\n",
    "    audio_features = extract_audio_features(audio_output_path) \n",
    "\n",
    "    # Load annotations\n",
    "    annotations, info = load_annotations(anno_file, info_file)\n",
    "    \n",
    "    # return visual_features, frames\n",
    "    return annotations,audio_features,visual_features,frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove all zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compact_non_zeros(array):\n",
    "    \"\"\"\n",
    "    Move all non-zero elements of an array to the front, keeping their order.\n",
    "\n",
    "    :param array: Input array.\n",
    "    :return: Compacted array with non-zeros at the front.\n",
    "    \"\"\"\n",
    "    non_zeros = array[array != 0]\n",
    "    if len(non_zeros) == 0:  # If the array is all zeros\n",
    "        print(\"All zeros\")\n",
    "        return array\n",
    "    return np.pad(non_zeros, (0, len(array) - len(non_zeros)), 'constant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kmeans and feature connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_and_cluster_features(video_path, anno_file, info_file, num_clusters=None):\n",
    "    \"\"\"\n",
    "    Integrate visual, audio, and annotation features from a video,\n",
    "    and perform clustering on the combined features.\n",
    "\n",
    "    :param video_path: Path to the video file.\n",
    "    :param anno_file: Path to the annotation file.\n",
    "    :param info_file: Path to the info file.\n",
    "    :param num_clusters: Number of clusters to use in KMeans.\n",
    "    :return: Cluster labels for each data point.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract data from video\n",
    "    # visual_features,frames=extractData(video_path, anno_file, info_file)\n",
    "\n",
    "    annotations,audio_features,visual_features,frames=extractData(video_path, anno_file, info_file)\n",
    "    \n",
    "    # If annotations include categorical data, convert it to numerical format\n",
    "    # May exclude it later \n",
    "    label_encoder = LabelEncoder()\n",
    "    categorical_columns = ['category']\n",
    "    for column in categorical_columns:\n",
    "        if column in annotations.columns:\n",
    "            annotations[column] = label_encoder.fit_transform(annotations[column])\n",
    "\n",
    "    # Exclude non-numerical data from annotations if necessary\n",
    "    annotation_features = annotations.drop(columns=['video_id']).values\n",
    "\n",
    "    \n",
    "    # \"\"\" \n",
    "\n",
    "    # Determine the original feature length\n",
    "    original_feature_length = len(visual_features) + len(audio_features) + len(annotation_features)\n",
    "\n",
    "    # Combine features with padding\n",
    "    combined_features = []\n",
    "    \n",
    "    # Remove temporaraily the audio feature\n",
    "    for i, frame in enumerate(frames):\n",
    "        \n",
    "        annotation_float_array = annotation2List(annotation_features[i])\n",
    "        \n",
    "        visual_padded, audio_padded, annotation_padded = padding(visual_features, audio_features, annotation_float_array)\n",
    "\n",
    "        combined_feature = np.concatenate([\n",
    "            np.array(visual_padded[i], dtype=float),\n",
    "            np.array(audio_padded, dtype=float),\n",
    "            np.array(annotation_padded[i], dtype=float),\n",
    "        ])\n",
    "        combined_features.append(combined_feature)\n",
    "\n",
    "    # Apply feature weighting\n",
    "    weighted_combined_features = apply_feature_weighting(combined_features, original_feature_length)\n",
    "\n",
    "    # Convert to 2D NumPy array and normalize\n",
    "    combined_features_array = np.array(weighted_combined_features)\n",
    "    \n",
    "    combined_features_normalized = StandardScaler().fit_transform(combined_features_array)\n",
    "    # \"\"\"\n",
    "    \n",
    "\n",
    "    # Calculate the number of clusters if not provided\n",
    "    if num_clusters is None:\n",
    "        num_clusters = floor(sqrt(len(combined_features_normalized)))\n",
    "    \n",
    "    \n",
    "    # Perform clustering\n",
    "    print(\"Performing clustering...\")\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(combined_features_normalized)\n",
    "    \n",
    "    return [frames, kmeans.labels_]\n",
    "    # return the frames and the labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find bigger cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def getBiggestCluster(labels):\n",
    "    cluster_counts = Counter(labels)\n",
    "\n",
    "    # Find the cluster with the maximum number of frames\n",
    "    return max(cluster_counts, key=cluster_counts.get),cluster_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frames Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_selection(frames, labels):\n",
    "    \"\"\"\n",
    "    Create a summary by selecting frames from the most populous cluster.\n",
    "\n",
    "    :param frames: List of frames.\n",
    "    :param labels: Cluster labels for each frame.\n",
    "    :return: List of frames belonging to the most populous cluster.\n",
    "    \"\"\"\n",
    "    # Count the number of frames in each cluster\n",
    "    max_cluster,cluster_counts = getBiggestCluster(labels)\n",
    "\n",
    "    # Select frames belonging to the most populous cluster\n",
    "    summary_frames = [frame for frame, label in zip(frames, labels) if label == max_cluster]\n",
    "\n",
    "    print(f\"Number of frames selected for summary: {len(summary_frames)}, Cluster: {max_cluster}\")\n",
    "    # print all clusters len\n",
    "    print(f\"Cluster counts: {cluster_counts}\")\n",
    "    return summary_frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video_from_frames(frames, output_path, frame_rate=25):\n",
    "    if not frames:\n",
    "        print(\"No frames to create a video.\")\n",
    "        return \n",
    "    # Determine the width and height from the first frame\n",
    "    height, width, layers = frames[0].shape\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, frame_rate, (width, height))\n",
    "\n",
    "    # Write each frame to the video\n",
    "    for frame in frames:\n",
    "        out.write(frame)\n",
    "\n",
    "    # Release the VideoWriter object\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load .Mat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_titles(encoded_titles, hdf5_file):\n",
    "    decoded_titles = []\n",
    "    for ref_array in encoded_titles:\n",
    "        # Handle the case where each ref_array might contain multiple references\n",
    "        for ref in ref_array:\n",
    "            # Dereference each HDF5 object reference to get the actual data\n",
    "            title_data = hdf5_file[ref]\n",
    "            # Decode the title\n",
    "            decoded_title = ''.join(chr(char[0]) for char in title_data)\n",
    "            decoded_titles.append(decoded_title)\n",
    "    return decoded_titles\n",
    "\n",
    "\n",
    "def load_mat_file(file_path,videoID):\n",
    "    \"\"\"\n",
    "    Load a .mat file and return its contents.\n",
    "\n",
    "    :param file_path: Path to the .mat file.\n",
    "    :return: Contents of the .mat file.\n",
    "    \"\"\"\n",
    "    with h5py.File(file_path, 'r') as file:\n",
    "        user_anno_refs=file['tvsum50']['user_anno'][:]\n",
    "        video_refs=file['tvsum50']['video'][:]\n",
    "\n",
    "        decoded_videos = decode_titles(video_refs,file)\n",
    "    \n",
    "        annotations = []\n",
    "        \n",
    "        # Get the index from decoded video list to find the annotation for the video\n",
    "        index = [i for i, x in enumerate(decoded_videos) if x.lower() in videoID.lower()][0]\n",
    "        \n",
    "        # Iterate over each reference\n",
    "        for ref in user_anno_refs:\n",
    "            # Dereference each HDF5 object reference\n",
    "            ref_data = file[ref[0]]\n",
    "\n",
    "            # Convert to NumPy array and add to the annotations list\n",
    "            annotations.append(np.array(ref_data))\n",
    "            \n",
    "        return annotations[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f1score with ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_f1score(ground_truth_path, summary_frames, videoID):\n",
    "    # Load ground truth summary\n",
    "    ground_truth = np.array(load_mat_file(ground_truth_path, videoID))\n",
    "    \n",
    "    summary_frames=np.array(summary_frames)\n",
    "    \n",
    "    # Average scores across annotators, if necessary\n",
    "    avg_ground_truth_scores = np.mean(ground_truth, axis=0) if ground_truth.ndim > 1 else ground_truth\n",
    "\n",
    "    # threshold\n",
    "    threshold = 1  # Ensure this threshold is suitable for your dataset\n",
    "    \n",
    "    # Convert ground truth to binary labels\n",
    "    binary_ground_truth = np.where(avg_ground_truth_scores >= threshold, 1, 0)\n",
    "\n",
    "    # Convert selected frames to binary labels\n",
    "    selected_frames_binary = np.zeros_like(binary_ground_truth)\n",
    "    \n",
    "    # Assuming summary_frames is a list of indices of frames selected for the summary\n",
    "    for index in summary_frames:\n",
    "        for frame_index in summary_frames:\n",
    "            selected_frames_binary[frame_index] = 1\n",
    "\n",
    "    # Calculate F1-Score\n",
    "    precision = precision_score(binary_ground_truth, selected_frames_binary)\n",
    "    recall = recall_score(binary_ground_truth, selected_frames_binary)\n",
    "    f1 = f1_score(binary_ground_truth, selected_frames_binary)\n",
    "    \n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1-Score:\", f1)\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to create summary video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_path='datasets/ydata-tvsum50-v1_1/data/ydata-tvsum50-anno.tsv'\n",
    "info_path='datasets/ydata-tvsum50-v1_1/data/ydata-tvsum50-info.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path='datasets/ydata-tvsum50-v1_1/video/'\n",
    "summary_video_path='datasets/summary_videos/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_path='datasets/ydata-tvsum50-v1_1/ground_truth/ydata-tvsum50.mat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get the list of the videos in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_list = [video for video in os.listdir(video_path) if video.endswith('.mp4')]  # List comprehension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for videoSummarizion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def videoSummarize(annotation_path=None, info_path=None, video_path=None, summary_video_path=None,video_list=None):\n",
    "    for video in video_list:  \n",
    "        frames,labels = integrate_and_cluster_features(video_path+video, annotation_path, info_path)\n",
    "        \n",
    "        # Select Representative Frames\n",
    "        summary_frames = frame_selection(frames, labels)  # Select frames from the most populous cluster\n",
    "        \n",
    "        # Create Summary Video\n",
    "        create_video_from_frames(summary_frames, summary_video_path+video,30) # Create a video from the selected frames\n",
    "        \n",
    "        kmeans_f1score(ground_truth_path,summary_frames,video.split('.')[0])\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for dynamic annotation and video summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 361ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 204ms/step\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 217ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 178ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 180ms/step\n",
      "1/1 [==============================] - 0s 189ms/step\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 173ms/step\n",
      "1/1 [==============================] - 0s 195ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 186ms/step\n",
      "1/1 [==============================] - 0s 188ms/step\n",
      "1/1 [==============================] - 0s 190ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 179ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 203ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 191ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 228ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 182ms/step\n",
      "1/1 [==============================] - 0s 181ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 174ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 160ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 170ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 159ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 161ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 183ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 166ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "MoviePy - Writing audio in datasets/extractedAudio/extracted_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Performing clustering...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Singleton array 3 cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\AUTH\\computer vision\\UnsupervisedVideoSummarization\\videoSum.ipynb Cell 48\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/AUTH/computer%20vision/UnsupervisedVideoSummarization/videoSum.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m videoSummarize(annotation_path, info_path, video_path, summary_video_path, video_list)\n",
      "\u001b[1;32md:\\AUTH\\computer vision\\UnsupervisedVideoSummarization\\videoSum.ipynb Cell 48\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/AUTH/computer%20vision/UnsupervisedVideoSummarization/videoSum.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvideoSummarize\u001b[39m(annotation_path\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, info_path\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, video_path\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, summary_video_path\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,video_list\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/AUTH/computer%20vision/UnsupervisedVideoSummarization/videoSum.ipynb#X54sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mfor\u001b[39;00m video \u001b[39min\u001b[39;00m video_list:  \n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/AUTH/computer%20vision/UnsupervisedVideoSummarization/videoSum.ipynb#X54sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         frames,labels \u001b[39m=\u001b[39m integrate_and_cluster_features(video_path\u001b[39m+\u001b[39;49mvideo, annotation_path, info_path)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/AUTH/computer%20vision/UnsupervisedVideoSummarization/videoSum.ipynb#X54sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39m# Select Representative Frames\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/AUTH/computer%20vision/UnsupervisedVideoSummarization/videoSum.ipynb#X54sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         summary_frames \u001b[39m=\u001b[39m frame_selection(frames, labels)  \u001b[39m# Select frames from the most populous cluster\u001b[39;00m\n",
      "\u001b[1;32md:\\AUTH\\computer vision\\UnsupervisedVideoSummarization\\videoSum.ipynb Cell 48\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/AUTH/computer%20vision/UnsupervisedVideoSummarization/videoSum.ipynb#X54sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m# get the prediction\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/AUTH/computer%20vision/UnsupervisedVideoSummarization/videoSum.ipynb#X54sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m kmeans\u001b[39m.\u001b[39mlabels_:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/AUTH/computer%20vision/UnsupervisedVideoSummarization/videoSum.ipynb#X54sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m     \u001b[39m# get the precision\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/AUTH/computer%20vision/UnsupervisedVideoSummarization/videoSum.ipynb#X54sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     precision\u001b[39m=\u001b[39mprecision_score(ground_truth,label,average\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmicro\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/AUTH/computer%20vision/UnsupervisedVideoSummarization/videoSum.ipynb#X54sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     \u001b[39m# get the recall\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/AUTH/computer%20vision/UnsupervisedVideoSummarization/videoSum.ipynb#X54sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m     recall\u001b[39m=\u001b[39mrecall_score(ground_truth,label,average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmicro\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\vasil\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1954\u001b[0m, in \u001b[0;36mprecision_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1825\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprecision_score\u001b[39m(\n\u001b[0;32m   1826\u001b[0m     y_true,\n\u001b[0;32m   1827\u001b[0m     y_pred,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1833\u001b[0m     zero_division\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwarn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1834\u001b[0m ):\n\u001b[0;32m   1835\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[0;32m   1836\u001b[0m \n\u001b[0;32m   1837\u001b[0m \u001b[39m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1952\u001b[0m \u001b[39m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[0;32m   1953\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1954\u001b[0m     p, _, _, _ \u001b[39m=\u001b[39m precision_recall_fscore_support(\n\u001b[0;32m   1955\u001b[0m         y_true,\n\u001b[0;32m   1956\u001b[0m         y_pred,\n\u001b[0;32m   1957\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[0;32m   1958\u001b[0m         pos_label\u001b[39m=\u001b[39;49mpos_label,\n\u001b[0;32m   1959\u001b[0m         average\u001b[39m=\u001b[39;49maverage,\n\u001b[0;32m   1960\u001b[0m         warn_for\u001b[39m=\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mprecision\u001b[39;49m\u001b[39m\"\u001b[39;49m,),\n\u001b[0;32m   1961\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1962\u001b[0m         zero_division\u001b[39m=\u001b[39;49mzero_division,\n\u001b[0;32m   1963\u001b[0m     )\n\u001b[0;32m   1964\u001b[0m     \u001b[39mreturn\u001b[39;00m p\n",
      "File \u001b[1;32mc:\\Users\\vasil\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1573\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1571\u001b[0m \u001b[39mif\u001b[39;00m beta \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1572\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mbeta should be >=0 in the F-beta score\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1573\u001b[0m labels \u001b[39m=\u001b[39m _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n\u001b[0;32m   1575\u001b[0m \u001b[39m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m samplewise \u001b[39m=\u001b[39m average \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msamples\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\vasil\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1374\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1371\u001b[0m \u001b[39mif\u001b[39;00m average \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m average_options \u001b[39mand\u001b[39;00m average \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1372\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39maverage has to be one of \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(average_options))\n\u001b[1;32m-> 1374\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m   1375\u001b[0m \u001b[39m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[0;32m   1376\u001b[0m \u001b[39m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m present_labels \u001b[39m=\u001b[39m unique_labels(y_true, y_pred)\u001b[39m.\u001b[39mtolist()\n",
      "File \u001b[1;32mc:\\Users\\vasil\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:86\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     60\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[39m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[0;32m     87\u001b[0m     type_true \u001b[39m=\u001b[39m type_of_target(y_true, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m     type_pred \u001b[39m=\u001b[39m type_of_target(y_pred, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_pred\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\vasil\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:394\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_consistent_length\u001b[39m(\u001b[39m*\u001b[39marrays):\n\u001b[0;32m    384\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check that all arrays have consistent first dimensions.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[39m    Checks whether all objects in arrays have the same shape or length.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[39m        Objects that will be checked for consistent length.\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 394\u001b[0m     lengths \u001b[39m=\u001b[39m [_num_samples(X) \u001b[39mfor\u001b[39;49;00m X \u001b[39min\u001b[39;49;00m arrays \u001b[39mif\u001b[39;49;00m X \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m]\n\u001b[0;32m    395\u001b[0m     uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\vasil\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:394\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_consistent_length\u001b[39m(\u001b[39m*\u001b[39marrays):\n\u001b[0;32m    384\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check that all arrays have consistent first dimensions.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[39m    Checks whether all objects in arrays have the same shape or length.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[39m        Objects that will be checked for consistent length.\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 394\u001b[0m     lengths \u001b[39m=\u001b[39m [_num_samples(X) \u001b[39mfor\u001b[39;00m X \u001b[39min\u001b[39;00m arrays \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]\n\u001b[0;32m    395\u001b[0m     uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[0;32m    396\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\vasil\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:335\u001b[0m, in \u001b[0;36m_num_samples\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m x\u001b[39m.\u001b[39mshape \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    334\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 335\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    336\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mSingleton array \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m cannot be considered a valid collection.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m x\n\u001b[0;32m    337\u001b[0m         )\n\u001b[0;32m    338\u001b[0m     \u001b[39m# Check that shape is returning an integer or default to len\u001b[39;00m\n\u001b[0;32m    339\u001b[0m     \u001b[39m# Dask dataframes may not return numeric shape[0] value\u001b[39;00m\n\u001b[0;32m    340\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], numbers\u001b[39m.\u001b[39mIntegral):\n",
      "\u001b[1;31mTypeError\u001b[0m: Singleton array 3 cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "videoSummarize(annotation_path, info_path, video_path, summary_video_path, video_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "##### only using visual features \n",
    "\n",
    "for index in summary_frames: \n",
    "    selected_frames_binary[index] = 1\n",
    "Precision: 0.06076388888888889\n",
    "Recall: 1.0\n",
    "F1-Score: 0.11456628477905074\n",
    "\n",
    "##### only audio+visual+annotation\n",
    "Precision: 0.06076388888888889\n",
    "Recall: 1.0\n",
    "F1-Score: 0.11456628477905074\n",
    "\n",
    "using PCA from 4 dim to 2dim in visual features\n",
    "Precision: 1.0\n",
    "Recall: 0.0013020833333333333\n",
    "F1-Score: 0.002600780234070221\n",
    "\n",
    "\n",
    "for index in summary_frames: \n",
    "        for frame_index in index:\n",
    "            selected_frames_binary[frame_index] = 1\n",
    "Precision: 1.0\n",
    "Recall: 0.037037037037037035\n",
    "F1-Score: 0.07142857142857142"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
