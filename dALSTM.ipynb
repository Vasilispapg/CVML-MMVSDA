{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles={}\n",
    "frames={}\n",
    "audios={}\n",
    "visuals={}\n",
    "objects={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "fol=os.listdir('/Users/macbookpro2017/Desktop/UnsupervisedVideoSummarization/video_ext_data')\n",
    "\n",
    "data=[]\n",
    "for videoID in fol:\n",
    "    files=os.listdir('/Users/macbookpro2017/Desktop/UnsupervisedVideoSummarization/video_ext_data/'+videoID)\n",
    "\n",
    "    for file in files:\n",
    "        if('model' not in file):\n",
    "            with open('/Users/macbookpro2017/Desktop/UnsupervisedVideoSummarization/video_ext_data/'+videoID+'/'+file,'rb') as f:\n",
    "                data=pickle.load(f)\n",
    "                if('title' in f.name):\n",
    "                    titles[videoID]=(data)\n",
    "                elif ('audio' in f.name):\n",
    "                    audios[videoID]=(data)\n",
    "                elif ('visual' in f.name):\n",
    "                    visuals[videoID]=(data)\n",
    "                elif ('objects' in f.name):\n",
    "                    objects[videoID]=(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from math import ceil,floor\n",
    "\n",
    "def calculate_mean_annotations(dir_anno, video_folder):\n",
    "    # Load annotations\n",
    "    df = pd.read_csv(dir_anno, sep='\\t', header=None, names=['videoID', 'category', 'annotations'])\n",
    "\n",
    "    # Get list of video IDs\n",
    "    video_ids = [f for f in os.listdir(video_folder)]\n",
    "    # Prepare result list\n",
    "    results = {}\n",
    "\n",
    "    # Calculate mean annotations for each video\n",
    "    for videoid in video_ids:\n",
    "        # Filter DataFrame for current video ID\n",
    "        video_df = df[df['videoID'] == videoid]\n",
    "\n",
    "        # If no annotations for this video, skip\n",
    "        if video_df.empty:\n",
    "            continue\n",
    "\n",
    "        # Process each frame\n",
    "        frame_means = []\n",
    "        for _, row in video_df.iterrows():\n",
    "            # Split the annotation string and convert to numeric\n",
    "            frame_means.append([float(a) for a in row['annotations'].split(',')])\n",
    "            \n",
    "        # Calculate mean of all annotators for this frame\n",
    "        frame_means=(np.array(frame_means).mean(axis=0)) #mean of all annotators for each frame\n",
    "        \n",
    "        # Round the mean to the nearest integer\n",
    "        for index,frame in enumerate(frame_means):\n",
    "            if abs(frame-ceil(frame))>0.5:\n",
    "                frame_means[index]=floor(frame)\n",
    "            else:\n",
    "                frame_means[index]=ceil(frame)\n",
    "\n",
    "        # Store in results\n",
    "        results[videoid] = frame_means\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "dir_anno = 'datasets/ydata-tvsum50-v1_1/data/ydata-tvsum50-anno.tsv'\n",
    "video_folder = '/Users/macbookpro2017/Desktop/UnsupervisedVideoSummarization/video_ext_data'\n",
    "mean_annotations = calculate_mean_annotations(dir_anno, video_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "def one_hot_encode(objects_in_frame, unique_objects):\n",
    "    # Handle None or empty values as 'no_object'\n",
    "    object_array = ['no_object' if x is None or len(x) == 0 else x for x in objects_in_frame]\n",
    "\n",
    "    # Reshape the array for OneHotEncoder\n",
    "    object_array = np.array(object_array).reshape(-1, 1)\n",
    "\n",
    "    # Initialize OneHotEncoder\n",
    "    encoder = OneHotEncoder(sparse_output=False,dtype=float, categories=[unique_objects])\n",
    "\n",
    "    # Fit and transform the data\n",
    "    one_hot_encoded = encoder.fit_transform(object_array)\n",
    "    return one_hot_encoded\n",
    "\n",
    "def detectObjects(objects):\n",
    "        \n",
    "    # Here, taking the first detected object\n",
    "    objects = [frame_objects[0] if frame_objects else 'None' for frame_objects in objects]\n",
    "\n",
    "    # Generate unique objects and include 'no_object'\n",
    "    unique_objects = sorted(set(objects + ['no_object']))\n",
    "\n",
    "    # One-hot encoding of objects\n",
    "    encoded_objects = one_hot_encode(objects, unique_objects)\n",
    "\n",
    "    # Padding not required as one-hot encoding ensures consistent vector length\n",
    "    \n",
    "    return encoded_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 22:27:44.343814: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "def train_lstm_model(combined_data_all, labels_all):\n",
    "    # Pad the sequences for LSTM\n",
    "    max_length = max(len(feature) for feature in combined_data_all)\n",
    "    padded_features = pad_sequences(combined_data_all, maxlen=max_length, padding='post', dtype='float32')\n",
    "\n",
    "    # Reshape labels if necessary\n",
    "    labels_all = np.array(labels_all)\n",
    "\n",
    "    # Define LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(max_length, padded_features.shape[2])))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Modify based on your problem\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  # Modify as needed\n",
    "\n",
    "    print('FIT MODEL STARTS')\n",
    "    # Train the model\n",
    "    model.fit(padded_features, labels_all, epochs=10, batch_size=32)\n",
    "\n",
    "    # Save the model\n",
    "    model.save('lstm_model.h5')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/macbookpro2017/Desktop/UnsupervisedVideoSummarization/dA.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macbookpro2017/Desktop/UnsupervisedVideoSummarization/dA.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         labels_all\u001b[39m.\u001b[39mappend(item[i])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macbookpro2017/Desktop/UnsupervisedVideoSummarization/dA.ipynb#W6sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Train LSTM model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/macbookpro2017/Desktop/UnsupervisedVideoSummarization/dA.ipynb#W6sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m lstm_model \u001b[39m=\u001b[39m train_lstm_model(combined_data_all, labels_all)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macbookpro2017/Desktop/UnsupervisedVideoSummarization/dA.ipynb#W6sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLSTM Complete.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macbookpro2017/Desktop/UnsupervisedVideoSummarization/dA.ipynb#W6sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# Save the trained SVM model\u001b[39;00m\n",
      "\u001b[1;32m/Users/macbookpro2017/Desktop/UnsupervisedVideoSummarization/dA.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macbookpro2017/Desktop/UnsupervisedVideoSummarization/dA.ipynb#W6sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Define LSTM model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macbookpro2017/Desktop/UnsupervisedVideoSummarization/dA.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m model \u001b[39m=\u001b[39m Sequential()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/macbookpro2017/Desktop/UnsupervisedVideoSummarization/dA.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m model\u001b[39m.\u001b[39madd(LSTM(\u001b[39m50\u001b[39m, input_shape\u001b[39m=\u001b[39m(max_length, padded_features\u001b[39m.\u001b[39;49mshape[\u001b[39m2\u001b[39;49m])))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macbookpro2017/Desktop/UnsupervisedVideoSummarization/dA.ipynb#W6sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m model\u001b[39m.\u001b[39madd(Dense(\u001b[39m1\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m'\u001b[39m))  \u001b[39m# Modify based on your problem\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/macbookpro2017/Desktop/UnsupervisedVideoSummarization/dA.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])  \u001b[39m# Modify as needed\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from joblib import dump\n",
    "\n",
    "\n",
    "combined_data_all = []\n",
    "labels_all = []\n",
    "\n",
    "# Combine all data\n",
    "for key, item in mean_annotations.items():\n",
    "    title = titles[key]\n",
    "    audio = audios[key]\n",
    "    visual = visuals[key]\n",
    "    object = objects[key]\n",
    "    \n",
    "    padded_encoded_objects = detectObjects(object)\n",
    "    combined_data=[]\n",
    "    for i, frame in enumerate(audio):\n",
    "        combined_feature = np.concatenate([\n",
    "            np.array(visual[i], dtype=float),\n",
    "            np.array(audio[i], dtype=float),\n",
    "            np.array(padded_encoded_objects[i], dtype=float),\n",
    "            np.array(title, dtype=float).reshape(-1),\n",
    "        ])    \n",
    "        combined_data_all.append(combined_feature)\n",
    "        labels_all.append(item[i])\n",
    "        \n",
    "        \n",
    "# Train LSTM model\n",
    "lstm_model = train_lstm_model(combined_data_all, labels_all)\n",
    "\n",
    "\n",
    "print(\"LSTM Complete.\")\n",
    "\n",
    "# Save the trained SVM model\n",
    "dump(lstm_model, 'model/trained_lstm_model.joblib')\n",
    "\n",
    "print(\"Model training complete and saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
